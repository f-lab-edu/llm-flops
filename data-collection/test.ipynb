{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents from blog website\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 138/138 [00:22<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from blog_data import WebsiteDataCrawler\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_loader = WebsiteDataCrawler()\n",
    "print(\"Retrieving documents from blog website\")\n",
    "docs = blog_loader.get_all_docs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=int(os.getenv(\"DOC_CHUNK_SIZE\")), chunk_overlap=int(os.getenv(\"DOC_CHUNK_OVERLAP\"))\n",
    ")\n",
    "split_doc = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13866\n"
     ]
    }
   ],
   "source": [
    "print(len(split_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    split_doc,\n",
    "    embeddings,\n",
    "    opensearch_url=\"https://localhost:9200\",\n",
    "    http_auth=(\"admin\", \"Open-search1!\"),\n",
    "    use_ssl = False,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    bulk_size=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about blog posts by several companies such as Anthropic on current events and new technologies in LLM field.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgenticRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Prompt[rlm/rag-prompt]********************\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    print(messages)\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "[HumanMessage(content='Anthropic에서 최근 LLM과 관련해서 어떤 연구를 하고 있는지 알려줘.', additional_kwargs={}, response_metadata={}, id='1a1a43b4-af07-4688-bcd4-9552cf55b160')]\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_O7fyRUDsY46DIEVXQIdSeYiM', 'function': {'arguments': '{\"query\":\"Anthropic recent research on LLM\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_81dd8129df'}, id='run-06916865-d041-4df2-b6ba-b22d52e758e6-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'Anthropic recent research on LLM'}, 'id': 'call_O7fyRUDsY46DIEVXQIdSeYiM', 'type': 'tool_call'}])]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='<h2 id=\"21-lora의-아이디어\">2.1. LoRA의 아이디어</h2>\\n\\n<p>미세조정은 LLM이 여태 학습한걸 뒤엎어버리는 대규모의 변화보다는 원하는 작업을 위해 LLM을 섬세하게 조정하는 과정을 묘사하는 단어입니다. LLM이 사전학습한 능력을 시작점으로 하기 때문에 같은 데이터양으로도 학습시켰을 때 훨씬 나은 성능을 기대할 수 있는거잖아요? LLM을 등에 업고 원하는 작업(downstream task)을 학습하는 일은 생각보다 어렵지 않은 일일지도 모릅니다.</p>\\n\\n<p><strong>그렇다면 미세조정을 하는데에 LLM이 포함한 수만큼 많은 파라미터를 학습하는건 너무 과한게 아닐까?</strong> 이 질문이 LoRA 방법론의 핵심에 있는 아이디어입니다.</p>\\n\\n<p>이 아이디어와 큰 행렬을 작은 두 개의 행렬곱으로 바꾸는 것은 어떻게 연결될까요? 아래의 예시를 봅시다.</p>\\n\\n<p><img src=\"/ncresearch/assets/img/post/c8416dcc21d8aad7f0ee65eaa47ad53854578b59/10_ezpz.png\" alt=\"\" /></p>\\n\\n<p>이 문제에 답을 제출할 때, 최대한 작은 행렬을 답으로 하고 싶다면 3번과 같이 열이 1개인 행렬을 선택할 수 있습니다. 따라서 이 문제를 푸는데 필요한 최소 열의 갯수 (n) 는 1개입니다. 5 x 5 크기의 행렬도 답안이 될 수 있지만 5 x 1 행렬로 답을 대신할 수도 있다는 뜻이죠.<span class=\"sidenote-number\"><small class=\"sidenote\">여기서 n을 intrinsic dimension이라고 합니다</small></span> 미세조정을 하는 건 이 문제보다는 어렵겠지만, 대충 비슷하다고 생각해봅시다.  <br />\\n이쯤에서 수식을 다시 한 번 봅시다.</p>\\n\\n<p>진화 알고리즘에 대해서는 LLM 등장 이전부터 연구되었던 일종의 네트워크 구조 탐색 방법 연구로 소개드릴 수 있을 것 같습니다. 최근의 LLM 연구는 학습 데이터 구축, Prompt engineering, RLHF/DPO 등으로 대표되는 학습 방법 고도화 등이 주류가 되었는데, 이 논문에서는 <strong>Model merge + Evolutionary algorithm (=Evolutionary Model Merge)</strong>이라는 LLM 등장 이전의 연구들을 조합한 새로운 방향을 제시하고 있습니다.</p>\\n\\n<p>모델 병합 과정의 자동화/고도화 외에 이 연구에서 내세우는 또다른 기여 포인트로는 <strong>Cross-Domain Merging</strong>이 있습니다. 단순히 성능 향상을 목표로 하는 것이 아니라 병합 전의 각 모델이 가지고 있는 강점을 통합 후의 모델이 손실 없이 물려받거나, 오히려 더 높은 능력을 가지게 되는 것을 말합니다. 논문에서는 일본어 능력을 가진 LLM과 수학 능력을 가진 LLM을 병합한 최종 모델에서 두 능력 모두 향상되는 것이 가능함을 실험적으로 보여주었습니다.</p>\\n\\n<h2 id=\"제안-방법-설명\">제안 방법 설명</h2>\\n\\n<p class=\"center_div\"><img src=\"/ncresearch/assets/img/post/c89a492c815109c0979ce0d4fa0efababd20e6b5/%EA%B7%B8%EB%A6%BC2.png\" alt=\"\" /></p>\\n<p class=\"center_div\"><em>그림 2. PS, DFS 모델 병합</em></p>\\n\\n<p>선택지가 없으면 고민할 것도 없습니다. 하나의 LLM만 잘 만들어서 그걸 쓰면 될텐데 왜 고민을 할까요? 안타깝게도, LLM을 개선하는 많은 방법들은 모든 것들을 순차적으로 적용할 수 있는 것도 아니고, 모든 것들이 상호 보완적이지도 않습니다. 그러니까 곧 하나의 잘 만든 LLM을 위해서 수 많은 선택이 필연적으로 뒤따릅니다. 예를 들어, 어떤 사용처에 맞추어 LLM을 개발하기로 했다고 생각해봅시다. 먼저 우리는 사전학습만 된 모델부터 시작할지, 지시문 학습 (instruction tuning) 된 모델로부터 시작할지부터 고민해야합니다. 그리고 그것을 미세조정 할지 말지, 데이터는 어떤 걸 쓸지, 학습은 어떻게 할지, 얼마나 할지 그리고 만약에 이걸 다 거친 후에는 어떻게  프롬프팅 할지, 추론시 파라미터는 뭘 줄지… 끝도 없는 선택의 향연입니다. 이런 선택지를 개략적으로 표로 그려보면 아래와 같습니다.</p>\\n\\n<p><br /></p>\\n\\n<h1 id=\"마치며\">마치며</h1>\\n\\n<p>최근의 NLP 연구는 대규모 언어 모델(Large Language Model, LLM)을 활용하여 범용 AI에 가까운 모델을 구현하는 방향이 주목받고 있습니다만, LLM에서의 Fine-tuning의 중요도 또한 결코 낮지 않으며 LLM 등장 이전의 상대적으로 작은 모델을 Task 특화 Fine-tuning하는 전통적인 학습 방식도 비용 효율성을 고려할 때 여전히 유효합니다. 오늘 소개해 드린 Self-distillation 관련 방법론들은 Fine-tuning 학습의 고도화라는 관점에서 충분히 가치가 있고 실용성도 상당히 높다고 생각합니다. 저희 생성번역기술실에서는 Self-distillation 기술을 포함하여 다양한 관점의 Fine-tuning 기법을 적용하여 한정된 자원 상황에서 최적의 성능을 달성할 수 있도록 연구 및 개발을 계속하여 진행하겠습니다.</p>\\n\\n<p><br /></p>\\n\\n<h1 id=\"references\">References</h1>', name='retrieve_blog_posts', id='1cf232ba-cbb6-49ee-97e1-c94040f9fe3e', tool_call_id='call_O7fyRUDsY46DIEVXQIdSeYiM')]}\n",
      "'\\n---\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Output from node 'rewrite':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='What recent research has Anthropic been conducting in relation to Large Language Models (LLMs)?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0125-preview'}, id='run-0e12bb47-af4d-4710-bcbe-5b8b0021577e-0')]}\n",
      "'\\n---\\n'\n",
      "---CALL AGENT---\n",
      "[HumanMessage(content='Anthropic에서 최근 LLM과 관련해서 어떤 연구를 하고 있는지 알려줘.', additional_kwargs={}, response_metadata={}, id='1a1a43b4-af07-4688-bcd4-9552cf55b160'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_O7fyRUDsY46DIEVXQIdSeYiM', 'function': {'arguments': '{\"query\":\"Anthropic recent research on LLM\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_81dd8129df'}, id='run-06916865-d041-4df2-b6ba-b22d52e758e6-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'Anthropic recent research on LLM'}, 'id': 'call_O7fyRUDsY46DIEVXQIdSeYiM', 'type': 'tool_call'}]), ToolMessage(content='<h2 id=\"21-lora의-아이디어\">2.1. LoRA의 아이디어</h2>\\n\\n<p>미세조정은 LLM이 여태 학습한걸 뒤엎어버리는 대규모의 변화보다는 원하는 작업을 위해 LLM을 섬세하게 조정하는 과정을 묘사하는 단어입니다. LLM이 사전학습한 능력을 시작점으로 하기 때문에 같은 데이터양으로도 학습시켰을 때 훨씬 나은 성능을 기대할 수 있는거잖아요? LLM을 등에 업고 원하는 작업(downstream task)을 학습하는 일은 생각보다 어렵지 않은 일일지도 모릅니다.</p>\\n\\n<p><strong>그렇다면 미세조정을 하는데에 LLM이 포함한 수만큼 많은 파라미터를 학습하는건 너무 과한게 아닐까?</strong> 이 질문이 LoRA 방법론의 핵심에 있는 아이디어입니다.</p>\\n\\n<p>이 아이디어와 큰 행렬을 작은 두 개의 행렬곱으로 바꾸는 것은 어떻게 연결될까요? 아래의 예시를 봅시다.</p>\\n\\n<p><img src=\"/ncresearch/assets/img/post/c8416dcc21d8aad7f0ee65eaa47ad53854578b59/10_ezpz.png\" alt=\"\" /></p>\\n\\n<p>이 문제에 답을 제출할 때, 최대한 작은 행렬을 답으로 하고 싶다면 3번과 같이 열이 1개인 행렬을 선택할 수 있습니다. 따라서 이 문제를 푸는데 필요한 최소 열의 갯수 (n) 는 1개입니다. 5 x 5 크기의 행렬도 답안이 될 수 있지만 5 x 1 행렬로 답을 대신할 수도 있다는 뜻이죠.<span class=\"sidenote-number\"><small class=\"sidenote\">여기서 n을 intrinsic dimension이라고 합니다</small></span> 미세조정을 하는 건 이 문제보다는 어렵겠지만, 대충 비슷하다고 생각해봅시다.  <br />\\n이쯤에서 수식을 다시 한 번 봅시다.</p>\\n\\n<p>진화 알고리즘에 대해서는 LLM 등장 이전부터 연구되었던 일종의 네트워크 구조 탐색 방법 연구로 소개드릴 수 있을 것 같습니다. 최근의 LLM 연구는 학습 데이터 구축, Prompt engineering, RLHF/DPO 등으로 대표되는 학습 방법 고도화 등이 주류가 되었는데, 이 논문에서는 <strong>Model merge + Evolutionary algorithm (=Evolutionary Model Merge)</strong>이라는 LLM 등장 이전의 연구들을 조합한 새로운 방향을 제시하고 있습니다.</p>\\n\\n<p>모델 병합 과정의 자동화/고도화 외에 이 연구에서 내세우는 또다른 기여 포인트로는 <strong>Cross-Domain Merging</strong>이 있습니다. 단순히 성능 향상을 목표로 하는 것이 아니라 병합 전의 각 모델이 가지고 있는 강점을 통합 후의 모델이 손실 없이 물려받거나, 오히려 더 높은 능력을 가지게 되는 것을 말합니다. 논문에서는 일본어 능력을 가진 LLM과 수학 능력을 가진 LLM을 병합한 최종 모델에서 두 능력 모두 향상되는 것이 가능함을 실험적으로 보여주었습니다.</p>\\n\\n<h2 id=\"제안-방법-설명\">제안 방법 설명</h2>\\n\\n<p class=\"center_div\"><img src=\"/ncresearch/assets/img/post/c89a492c815109c0979ce0d4fa0efababd20e6b5/%EA%B7%B8%EB%A6%BC2.png\" alt=\"\" /></p>\\n<p class=\"center_div\"><em>그림 2. PS, DFS 모델 병합</em></p>\\n\\n<p>선택지가 없으면 고민할 것도 없습니다. 하나의 LLM만 잘 만들어서 그걸 쓰면 될텐데 왜 고민을 할까요? 안타깝게도, LLM을 개선하는 많은 방법들은 모든 것들을 순차적으로 적용할 수 있는 것도 아니고, 모든 것들이 상호 보완적이지도 않습니다. 그러니까 곧 하나의 잘 만든 LLM을 위해서 수 많은 선택이 필연적으로 뒤따릅니다. 예를 들어, 어떤 사용처에 맞추어 LLM을 개발하기로 했다고 생각해봅시다. 먼저 우리는 사전학습만 된 모델부터 시작할지, 지시문 학습 (instruction tuning) 된 모델로부터 시작할지부터 고민해야합니다. 그리고 그것을 미세조정 할지 말지, 데이터는 어떤 걸 쓸지, 학습은 어떻게 할지, 얼마나 할지 그리고 만약에 이걸 다 거친 후에는 어떻게  프롬프팅 할지, 추론시 파라미터는 뭘 줄지… 끝도 없는 선택의 향연입니다. 이런 선택지를 개략적으로 표로 그려보면 아래와 같습니다.</p>\\n\\n<p><br /></p>\\n\\n<h1 id=\"마치며\">마치며</h1>\\n\\n<p>최근의 NLP 연구는 대규모 언어 모델(Large Language Model, LLM)을 활용하여 범용 AI에 가까운 모델을 구현하는 방향이 주목받고 있습니다만, LLM에서의 Fine-tuning의 중요도 또한 결코 낮지 않으며 LLM 등장 이전의 상대적으로 작은 모델을 Task 특화 Fine-tuning하는 전통적인 학습 방식도 비용 효율성을 고려할 때 여전히 유효합니다. 오늘 소개해 드린 Self-distillation 관련 방법론들은 Fine-tuning 학습의 고도화라는 관점에서 충분히 가치가 있고 실용성도 상당히 높다고 생각합니다. 저희 생성번역기술실에서는 Self-distillation 기술을 포함하여 다양한 관점의 Fine-tuning 기법을 적용하여 한정된 자원 상황에서 최적의 성능을 달성할 수 있도록 연구 및 개발을 계속하여 진행하겠습니다.</p>\\n\\n<p><br /></p>\\n\\n<h1 id=\"references\">References</h1>', name='retrieve_blog_posts', id='1cf232ba-cbb6-49ee-97e1-c94040f9fe3e', tool_call_id='call_O7fyRUDsY46DIEVXQIdSeYiM'), AIMessage(content='What recent research has Anthropic been conducting in relation to Large Language Models (LLMs)?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0125-preview'}, id='run-0e12bb47-af4d-4710-bcbe-5b8b0021577e-0')]\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='Anthropic has been exploring several innovative approaches in the field of Large Language Models (LLMs). Here are some key areas of their recent research:\\n\\n1. **LoRA (Low-Rank Adaptation)**: This methodology focuses on fine-tuning LLMs in a way that minimizes the disruption of what the model has already learned. Instead of retraining numerous parameters, LoRA proposes adjusting a smaller subset of parameters, which can be represented as a product of two smaller matrices. This approach aims to maintain the intrinsic capabilities of the LLM while efficiently adapting it to specific tasks.\\n\\n2. **Evolutionary Algorithms and Model Merging**: Anthropic is combining traditional evolutionary algorithms with model merging techniques to create new LLM architectures. This includes the concept of \"Evolutionary Model Merge,\" which integrates the strengths of different models into a single more capable model. For example, merging an LLM with proficiency in Japanese with another that excels in mathematical tasks, resulting in a model that enhances both capabilities.\\n\\n3. **Cross-Domain Merging**: This research focuses on merging models from different domains without losing the individual strengths of each model. The goal is to enhance overall performance and capabilities beyond what each model could achieve independently.\\n\\n4. **Fine-tuning and Self-distillation**: Anthropic continues to value the traditional fine-tuning approaches for LLMs, considering them cost-effective and still relevant. They are working on enhancing fine-tuning methodologies, including self-distillation techniques, to achieve optimal performance with limited resources.\\n\\nThese research initiatives highlight Anthropic\\'s commitment to advancing the functionality and efficiency of LLMs through innovative adaptation and merging strategies, ensuring that the models not only retain their pre-trained capabilities but also gain new functionalities tailored to specific tasks.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-turbo-2024-04-09', 'system_fingerprint': 'fp_68a5bb159e'}, id='run-2a4f39c9-d404-4fa3-a070-b76360c0cc6b-0')]}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Anthropic에서 최근 LLM과 관련해서 어떤 연구를 하고 있는지 알려줘.\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCsoft는 최근에 다음과 같은 기술들을 연구하고 발표했습니다:\n",
      "\n",
      "1\n",
      " **사용자 정의 호출어 인식 기술 (UDKWS)**: 이 기술은 사용자가 자신만의 호출어를 설정할 수 있게 해주는 음성 인식 기술입니다\n",
      " 이 기술은 디바이스 내부에서 동작하며, 빠른 응답 시간과 가벼운 구조를 목표로 하고 있습니다\n",
      " 이를 통해 다양한 디바이스에서 제한 없이 사용할 수 있도록 개발되었습니다\n",
      "\n",
      "\n",
      "이 기술들은 NCsoft가 사용자 경험을 향상시키고, 기술적 한계를 극복하기 위해 지속적으로 연구하고 개발하는 노력의 일환입니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sen in output['agent']['messages'][0].content.split('.'):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAG (Corrective RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.2\"\n",
    "model_tested = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>안녕하세요!\n",
      "NLP센터 금융언어이해팀 민중현 입니다.\n",
      "NLP Tech 블로그를 통해 다시 한 번 이야기 나눌 수 있어서 기쁩니다.\n",
      "오늘은 논문 소개가 아닌, 센터 내에서 진행한 연구를 소개하고자 합니다.</p>\n",
      "\n",
      "<h3 id=\"지금은-바야흐로\">지금은 바야흐로</h3>\n",
      "\n",
      "<p>…트랜스포머 기반 LLM의 시대!</p>\n",
      "\n",
      "<p align=\"center\">\n",
      "<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm1.png\" />\n",
      "<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm2.png\" />\n",
      "<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm3.png\" />\n",
      "</p>\n",
      "\n",
      "<p>최근 출시된 LLM들, 그리고 특히 그 중 생성형 모델들은 참 놀랍습니다. 정량적으로 나타나는 번역, 요약등 여러 과제에서의 정량적 성능뿐만 아니라, 사람이 쓴 것과 비슷한 이메일, 게임 스토리, 이력서, 편지, 연설문도 쓸 수 있다고 하지요. 그 뿐만 아니라, 미국 대학수학능력검정시험 (SAT), 변호사 시험, 그리고 의사 면허 시험 등도 통과한다고 전해집니다. 그렇다면, 드디어! 인간과 같은 수준으로 언어를 습득/처리할 수 있는 컴퓨터 시스템 (human-like computational language processing system) 이 만들어 진 걸까요?</p>\n",
      "\n",
      "<p>Short answer: No</p>\n",
      "{'score': '1'}\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a teacher grading a quiz. You will be given: \n",
    "    1/ a QUESTION\n",
    "    2/ A FACT provided by the student\n",
    "    \n",
    "    You are grading RELEVANCE RECALL:\n",
    "    A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION. \n",
    "    A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION. \n",
    "    1 is the highest (best) score. 0 is the lowest score you can give. \n",
    "    \n",
    "    Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct. \n",
    "    \n",
    "    Avoid simply stating the correct answer at the outset.\n",
    "    \n",
    "    Question: {question} \\n\n",
    "    Fact: \\n\\n {documents} \\n\\n\n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"네이버 최근 LLM 연구\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(doc_txt)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"documents\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://ncsoft.github.io/ncresearch/c8416dcc21d8aad7f0ee65eaa47ad53854578b59', 'title': 'Large Language Model을 밀어서 잠금해제: Parameter-Efficient Fine-Tuning 2 | \\u3000', 'description': 'Parameter-Efficient Fine-Tuning (PEFT) 방법들', 'language': 'en'}, page_content='<h2 id=\"21-lora의-아이디어\">2.1. LoRA의 아이디어</h2>\\n\\n<p>미세조정은 LLM이 여태 학습한걸 뒤엎어버리는 대규모의 변화보다는 원하는 작업을 위해 LLM을 섬세하게 조정하는 과정을 묘사하는 단어입니다. LLM이 사전학습한 능력을 시작점으로 하기 때문에 같은 데이터양으로도 학습시켰을 때 훨씬 나은 성능을 기대할 수 있는거잖아요? LLM을 등에 업고 원하는 작업(downstream task)을 학습하는 일은 생각보다 어렵지 않은 일일지도 모릅니다.</p>\\n\\n<p><strong>그렇다면 미세조정을 하는데에 LLM이 포함한 수만큼 많은 파라미터를 학습하는건 너무 과한게 아닐까?</strong> 이 질문이 LoRA 방법론의 핵심에 있는 아이디어입니다.</p>\\n\\n<p>이 아이디어와 큰 행렬을 작은 두 개의 행렬곱으로 바꾸는 것은 어떻게 연결될까요? 아래의 예시를 봅시다.</p>\\n\\n<p><img src=\"/ncresearch/assets/img/post/c8416dcc21d8aad7f0ee65eaa47ad53854578b59/10_ezpz.png\" alt=\"\" /></p>\\n\\n<p>이 문제에 답을 제출할 때, 최대한 작은 행렬을 답으로 하고 싶다면 3번과 같이 열이 1개인 행렬을 선택할 수 있습니다. 따라서 이 문제를 푸는데 필요한 최소 열의 갯수 (n) 는 1개입니다. 5 x 5 크기의 행렬도 답안이 될 수 있지만 5 x 1 행렬로 답을 대신할 수도 있다는 뜻이죠.<span class=\"sidenote-number\"><small class=\"sidenote\">여기서 n을 intrinsic dimension이라고 합니다</small></span> 미세조정을 하는 건 이 문제보다는 어렵겠지만, 대충 비슷하다고 생각해봅시다.  <br />\\n이쯤에서 수식을 다시 한 번 봅시다.</p>'),\n",
       " Document(metadata={'source': 'https://ncsoft.github.io/ncresearch/195e5588d76145a5becc6052cdbf9cfa5092dcbe', 'title': '문장 부호 복원을 통한 구조 이해 능력의 비지도 향상 | \\u3000', 'description': '선형적인 단어 예측이 언어모델 사전 학습에 최선일까? 문장부호 복원을 통한 언어 구조 학습을 제시한다.', 'language': 'en'}, page_content='<p>안녕하세요!\\nNLP센터 금융언어이해팀 민중현 입니다.\\nNLP Tech 블로그를 통해 다시 한 번 이야기 나눌 수 있어서 기쁩니다.\\n오늘은 논문 소개가 아닌, 센터 내에서 진행한 연구를 소개하고자 합니다.</p>\\n\\n<h3 id=\"지금은-바야흐로\">지금은 바야흐로</h3>\\n\\n<p>…트랜스포머 기반 LLM의 시대!</p>\\n\\n<p align=\"center\">\\n<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm1.png\" />\\n<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm2.png\" />\\n<img src=\"assets/img/post/195e5588d76145a5becc6052cdbf9cfa5092dcbe/llm3.png\" />\\n</p>\\n\\n<p>최근 출시된 LLM들, 그리고 특히 그 중 생성형 모델들은 참 놀랍습니다. 정량적으로 나타나는 번역, 요약등 여러 과제에서의 정량적 성능뿐만 아니라, 사람이 쓴 것과 비슷한 이메일, 게임 스토리, 이력서, 편지, 연설문도 쓸 수 있다고 하지요. 그 뿐만 아니라, 미국 대학수학능력검정시험 (SAT), 변호사 시험, 그리고 의사 면허 시험 등도 통과한다고 전해집니다. 그렇다면, 드디어! 인간과 같은 수준으로 언어를 습득/처리할 수 있는 컴퓨터 시스템 (human-like computational language processing system) 이 만들어 진 걸까요?</p>\\n\\n<p>Short answer: No</p>'),\n",
       " Document(metadata={'source': 'https://ncsoft.github.io/ncresearch/c8416dcc21d8aad7f0ee65eaa47ad53854578b59', 'title': 'Large Language Model을 밀어서 잠금해제: Parameter-Efficient Fine-Tuning 2 | \\u3000', 'description': 'Parameter-Efficient Fine-Tuning (PEFT) 방법들', 'language': 'en'}, page_content='<p><br /></p>\\n\\n<p>이렇게만 된다면 미세조정에 필요한 파라미터 수가 LLM이 원래 포함한 것보다 훨씬 적어질 수도 있겠군요?  <br />\\n이렇게만 된다면…말이죠?  <br />\\n¯\\\\_(ツ)_/¯</p>\\n\\n<p><img src=\"/ncresearch/assets/img/post/c8416dcc21d8aad7f0ee65eaa47ad53854578b59/12_alphago_smash.png\" alt=\"\" /></p>\\n\\n<p>제가 깔끔하다고 생각한 이유는 이게 아님말고 식의 가설이 아니라는 점입니다. 미세조정으로 같은 성능에 도달하기 위해 필요한 파라미터 수는 사전학습 모델의 그것보다 훨씬 작다는 사실이 이미 보고되어있으며 <sup id=\"fnref:11\" role=\"doc-noteref\"><a href=\"#fn:11\" class=\"footnote\" rel=\"footnote\">11</a></sup>이 내용들이 LoRA의 핵심 가설을 뒷받침하는 근거로 제시되고 있습니다.</p>\\n\\n<p><br /></p>\\n\\n<p>이런 원리로 LoRA는 학습할 파라미터의 수를 LLM을 그냥 미세조정할 때 대비 <strong>0.01%</strong> 수준까지 줄일 수 있었습니다. 두 개의 작은 <strong>행렬곱을 학습</strong><span class=\"sidenote-number\"><small class=\"sidenote\">보통 신경망의 각 층에는 행렬곱 (선형 연산) 뿐만 아니라 비선형연산 등이 포함되어 훨씬 복잡하다. Adapter가 복잡한 케이스라면 LoRA에서 하는 연산은 선형 연산이므로 부담이 적다.</small></span>하는 걸로요.</p>\\n\\n<p><br /></p>\\n\\n<h2 id=\"22-실제-효과\">2.2. 실제 효과</h2>\\n\\n<h3 id=\"gpu-메모리-절약-수준\">GPU 메모리 절약 수준</h3>'),\n",
       " Document(metadata={'source': 'https://ncsoft.github.io/ncresearch/8406a783d8dea98e70cbccaa329e505931d8f378', 'title': 'VARCO-MLLM 한국어 잘하는 멀티모달 모델 | \\u3000', 'description': 'MLLM의 기본 개념을 설명하고, VARCO-MLLM을 소개합니다.', 'language': 'en'}, page_content='</ul>')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the latest research on LLM from Naver. \n",
      "\n",
      "The provided documents seem to be related to a research paper titled \"Large Language Model을 밀어서 잠금해제: Parameter-Efficient Fine-Tuning 2\" which discusses the LoRA (Low-Rank Adaptation) method for parameter-efficient fine-tuning of large language models.\n",
      "\n",
      "LoRA is a method that reduces the number of parameters required for fine-tuning by using low-rank adaptations, which can lead to significant memory and computational savings.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    \n",
    "    Use the following documents to answer the question. \n",
    "    \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Documents: {documents} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-mlops-gR6V-zHB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
