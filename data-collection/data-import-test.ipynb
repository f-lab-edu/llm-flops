{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Google has developed a system that enables an LLM to undergo \"edit cycles\" before generating a final response. o1 but for creative writing in notebookLM?', 'post_score': 114, 'post_id': '1fwh4y2', 'post_url': 'https://v.redd.it/bc9o56epuusd1', 'post_author': Redditor(name='onil_gova')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Open sourcing Grok 2 with the release of Grok 3, just like we did with Grok 1!', 'post_score': 466, 'post_id': '1fw7ikv', 'post_url': 'https://x.com/elonmusk/status/1842248588149117013', 'post_author': Redditor(name='Nickism')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Llama Assistant v0.1.32 is released with the binary distributions for MacOS, Windows, and Linux.', 'post_score': 46, 'post_id': '1fwl0p4', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwl0p4/llama_assistant_v0132_is_released_with_the_binary/', 'post_author': Redditor(name='PuzzleheadedLab4175')}, page_content=\"https://reddit.com/link/1fwl0p4/video/u7avn2504wsd1/player\\n\\n# ü¶ô What's new with Llama Assistant this week? üöÄ\\n\\n* üî• Support for streaming response, much faster feedback!\\n* üéôÔ∏è Support for WhisperCPP - offline speech-to-text conversion, your voice won't leave your computer.\\n* üçé Packaged (binary) versions for MacOS, Windows, and Linux. Download at [https://github.com/vietanhdev/llama-assistant/releases/tag/v0.1.32](https://github.com/vietanhdev/llama-assistant/releases/tag/v0.1.32)\\n\\nüåüüåüüåü **Star the Repo for the updates:** [https://github.com/vietanhdev/llama-assistant](https://github.com/vietanhdev/llama-assistant) \"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Slightly overdid the question revision step, I think', 'post_score': 22, 'post_id': '1fwmg29', 'post_url': 'https://i.redd.it/7wii9vj8nwsd1.png', 'post_author': Redditor(name='Everlier')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': '60GB VRAM Ziptie Build for 2400‚Ç¨', 'post_score': 109, 'post_id': '1fwcu30', 'post_url': 'https://www.reddit.com/gallery/1fwcu30', 'post_author': Redditor(name='Sunija_Dev')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': \"I tested few TTS apps ‚Äì You can decide what's the best\", 'post_score': 13, 'post_id': '1fwn92m', 'post_url': 'https://v.redd.it/c7daserjxwsd1', 'post_author': Redditor(name='MustBeSomethingThere')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': '<Looks at watch> ü§®', 'post_score': 351, 'post_id': '1fw1vol', 'post_url': 'https://i.redd.it/ik47wnugcrsd1.jpeg', 'post_author': Redditor(name='Porespellar')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'My jank 2x 3090, 1x a4000 setup', 'post_score': 14, 'post_id': '1fwk815', 'post_url': 'https://www.reddit.com/gallery/1fwk815', 'post_author': Redditor(name='sammcj')}, page_content='I had room for the a4000 vertically but - it would cut the air to one of the 3090s and itself, so out rigged up a bracket with a pcie x16 extender and of course I had to use one cable tie to make it locallama worthy ;)'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'llama-swap: a proxy for llama.cpp to swap between models', 'post_score': 18, 'post_id': '1fwj055', 'post_url': 'https://github.com/mostlygeek/llama-swap', 'post_author': Redditor(name='No-Statement-0001')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': \"Wake up babe, ZLUDA's alive again\", 'post_score': 156, 'post_id': '1fw37xu', 'post_url': 'https://vosen.github.io/ZLUDA/blog/zludas-third-life/', 'post_author': Redditor(name='KillerX629')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Metas new image/video/audio generation models', 'post_score': 217, 'post_id': '1fvz7gj', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fvz7gj/metas_new_imagevideoaudio_generation_models/', 'post_author': Redditor(name='schlammsuhler')}, page_content='https://x.com/AIatMeta/status/1842188252541043075?t=RfKYKhV8KDHfOGYpZWUYiQ&s=19'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Best Inference Engines for Running LLama', 'post_score': 7, 'post_id': '1fwldl8', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwldl8/best_inference_engines_for_running_llama/', 'post_author': Redditor(name='TechnoAcc')}, page_content='I have been playing around with different inference engines for running LLama 3.1, I have tried out vllm, llama.cpp, took the pain to setup and test tensorrt llm ( it‚Äôs not a healthy thing to do)\\n\\nNone of these worked out well for me like SG Lang,  https://github.com/sgl-project/sglang by the guys behind LLM arena.\\n\\nIt is really fast, as fast as tensorrt llm and really easy to setup and deploy. I was able to deploy Llama 3.1 70 b quite easily on a vm.\\n\\nIt‚Äôs great to see tools like this coming out, these stuff is often only the privilege of the big labs.\\n\\nWhat other inference engines do you use and what are your experiences with them?'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'KV-Compress: KV cache compression for high-throughput LLM inference', 'post_score': 91, 'post_id': '1fw5ffm', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw5ffm/kvcompress_kv_cache_compression_for/', 'post_author': Redditor(name='isaacrehg')}, page_content='We\\'ve developed a new method for KV cache compression that can be used with PagedAttention to maximize LLM throughput, delivering up to 5x more toks/sec than vanilla vLLM.\\n\\nPaper:\\xa0[https://arxiv.org/abs/2410.00161](https://arxiv.org/abs/2410.00161)\\n\\nGithub (fork of vLLM):\\xa0[https://github.com/IsaacRe/vllm-kvcompress/tree/main](https://github.com/IsaacRe/vllm-kvcompress/tree/main)\\n\\nOur method works by removing blocks of under-utilized KVs from cache to free up space, letting you add more sequences to the decoding batch and increase total throughput. It\\'s especially useful for hardware configurations where available vRAM is scarce, eg. consumer GPUs.\\n\\nIf you\\'re using vLLM and want to improve your throughput (or are just interested in KV cache compression) try it out!\\n\\n**Getting Started**\\n\\nIt\\'s easy--just launch vLLM with \\n\\n    vllm serve <model_name> --enforce-eager --enable-kvc\\n\\nthen add *\"max\\\\_cache\\\\_tokens: xxx\"* to your sampling parameters to configure the maximum number of KVs to keep in cache for each request. Checkout our Github for more details.\\n\\n**Results Summary**\\n\\nWe benchmarked Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 on LongBench and reached compression rates of up to 64x while maintaining 90% of full-cache performance on all but three of its subsets (degradation is still observed for summarization tasks). At 8x compression performance impact is negligible for most subsets.\\n\\nAt 64x compression we observe over 5x throughput with the 8B model running on a single L4, and 2x throughput with the 70B model running on a single H100. At 8x we see more modest increases of 3.4x and 1.8x for the 8B and 70B model, respectively. See the paper for full results!\\n\\n[X/Twitter thread](https://x.com/isaacrehg/status/1841540805174956394)'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': \"Drummer's Tiger Gemma 9B v3 - Decensored differently.\", 'post_score': 72, 'post_id': '1fw72tv', 'post_url': 'https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v3', 'post_author': Redditor(name='TheLocalDrummer')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Reflection 70B Reproduction Failed', 'post_score': 3, 'post_id': '1fwo4qf', 'post_url': 'https://x.com/mattshumer_/status/1842313328166907995', 'post_author': Redditor(name='madredditscientist')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Underclocking GPUs to save on power costs?', 'post_score': 14, 'post_id': '1fwg537', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwg537/underclocking_gpus_to_save_on_power_costs/', 'post_author': Redditor(name='ApprehensiveDuck2382')}, page_content=\"tl;dr Can you underclock your GPUs to save substantially on electricity costs without greatly impacting inference speeds?\\n\\nCurrently, I'm using only one powerful Nvidia GPU, but it seems to be contributing quite a lot to high electricity bills when I run a lot of inference. I'd love to pick up another 1 or 2 value GPUs to run bigger models, but I'm worried about running up humongous bills.\\n\\nI've seen someone in one of these threads claim that Nvidia's prices for their enterprise server GPUs aren't justified by their much greater power efficiency, because you can just underclock a consumer GPU to achieve the same. Is that more-or-less true? What kind of wattage could you get a 3090 or 4090 down to without suffering too much speed loss on inference? How would I go about doing so? I'm reasonably technical, but I've never underclocked or overclocked anything.\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Meta Movie Gen - the most advanced media foundation AI models | AI at Meta', 'post_score': 157, 'post_id': '1fvzagc', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fvzagc/meta_movie_gen_the_most_advanced_media_foundation/', 'post_author': Redditor(name='Nunki08')}, page_content='‚û°Ô∏è\\xa0[https://ai.meta.com/research/movie-gen/](https://ai.meta.com/research/movie-gen/)\\n\\nhttps://reddit.com/link/1fvzagc/video/p4nzo93gsqsd1/player\\n\\nGenerate videos from text Edit video with text  \\nProduce personalized videos  \\nCreate sound effects and soundtracks\\n\\nPaper: MovieGen: A Cast of Media Foundation Models  \\n[https://ai.meta.com/static-resource/movie-gen-research-paper](https://ai.meta.com/static-resource/movie-gen-research-paper)\\n\\nSource: AI at Meta on X:\\xa0[https://x.com/AIatMeta/status/1842188252541043075](https://x.com/AIatMeta/status/1842188252541043075)\\n\\nhttps://preview.redd.it/vy2x92sisqsd1.jpg?width=474&format=pjpg&auto=webp&s=0e45da183ac4e29704adc84c5d25b694767fb8dc\\n\\n'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'TTS of large texts with translation in real-time using Llama 3.2 3B uncensored', 'post_score': 93, 'post_id': '1fw2pqd', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw2pqd/tts_of_large_texts_with_translation_in_realtime/', 'post_author': Redditor(name='Ok-Scarcity-7875')}, page_content=\"I wrote a nice script which can you read large texts in real time, like an audio book. Thanks to voice cloning the text is also read by anyone you want! On top you can also let the script translate the text in your desired language via a LLM.  \\nI used [https://huggingface.co/mradermacher/Llama-3.2-3B-Instruct-uncensored-GGUF](https://huggingface.co/mradermacher/Llama-3.2-3B-Instruct-uncensored-GGUF) with llama.cpp for this task.\\n\\nImagine you want to read something difficult in a foreign language, but you are both too lazy to read it and you're also have trouble understanding it because the text is not written in your native language.\\n\\nHere's the script [https://github.com/dynamiccreator/voice-text-reader](https://github.com/dynamiccreator/voice-text-reader)\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Is there a potential bug of latest llama.cpp not cleaning up memory after exiting?', 'post_score': 4, 'post_id': '1fwl8p5', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwl8p5/is_there_a_potential_bug_of_latest_llamacpp_not/', 'post_author': Redditor(name='aeroumbria')}, page_content='I have been noticing that recently my memory usage stays high after loading models with llamacpp, and each time I run a new model, a part of my memory is forever locked out until reboot, even after exiting llamacpp. This is observed on the latest version of the docker image \"ghcr.io/ggerganov/llama.cpp:server\", but I have seen this with the oobabooga version as well. It is definitely not regular caching / normal memory management because the memory usage increases the more times I run llamacpp and can definitely lead to system lag if I max out the memory usage. Llamacpp does unload the models on exit, but some parts of the memory usage is not returned. I use GPU layer offloading but often run with \"-nkvo\" if that is relevant.\\n\\nI\\'m running OpenSUSE Tumbleweed with nvidia driver 560.28.03. I am not familiar with diagnosing this type of memory issues, so I would like to hear if anyone else is also experiencing this before investigate further and report a bug.'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Optillm: An optimizing Inference Proxy with Plugins', 'post_score': 7, 'post_id': '1fwjeg9', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwjeg9/optillm_an_optimizing_inference_proxy_with_plugins/', 'post_author': Redditor(name='asankhs')}, page_content='[Optillm](https://github.com/codelion/optillm) is an optimizing inference proxy that has over a dozen [techniques](https://github.com/codelion/optillm?tab=readme-ov-file#implemented-techniques) that aim to improve the accuracy of the responses using test-time compute. Over the last couple of months we have set several [SOTA results](https://github.com/codelion/optillm?tab=readme-ov-file#sota-results-on-benchmarks-with-optillm) using smaller and less capable models like gpt-4o-mini.   \\n  \\nRecently, we have added support for [plugins](https://github.com/codelion/optillm?tab=readme-ov-file#implemented-plugins) that enable capabilities like memory, privacy and code execution to optillm. Plugins are just python scripts that you can also write yourself, optillm would then load them at start from the [directory](https://github.com/codelion/optillm/tree/main/optillm/plugins).   \\n  \\nYou can now also combine the plugins and techniques using & and | operators. E.g. We recently evaluated the new [FRAMES benchmark](https://huggingface.co/datasets/google/frames-benchmark) from Google. Using a combination of plugins and techniques (we used readurls&memory-gpt-4o-mini) we were able to get 65.7% accuracy on the benchmark which is very close to what Google reported in their paper with Gemini Flash 1.5 (66.5) which has a context length that is almost 10 times that of gpt-4o-mini.\\n\\nhttps://preview.redd.it/pfp89daijvsd1.png?width=1470&format=png&auto=webp&s=a5c4fba1056e9d00b4aae686b90d36f4fd9e177d\\n\\nDo check out Optillm at [https://github.com/codelion/optillm](https://github.com/codelion/optillm) '),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Would you use a Pinterest for UI/UX code? Prototyped this tonight out of my own need for a project', 'post_score': 8, 'post_id': '1fwih74', 'post_url': 'https://v.redd.it/outifbt39vsd1', 'post_author': Redditor(name='WhosAfraidOf_138')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Just another local inference build and its challenges', 'post_score': 4, 'post_id': '1fwmbr4', 'post_url': 'https://i.redd.it/5mvt1r93mwsd1.jpeg', 'post_author': Redditor(name='Chlorek')}, page_content='Flexing my double RTX 3090 build. Had occasional boot issues but resolved it by dropping PCIe gen from 4 to 3, despite riser being right for the job. Still need to find a method to mount the front card in a more trustworthy way. Btw I am not crazy enough to buy them from the store so got used ones for just below 1000 USD. Spare me noting that I should change my watercooling pipes, ikr :D I‚Äôm inferring locally for my own AI project, as a replacement for Copilot (autocompletion for programming) and also I can load NDA covered documents without worrying about it. Llama models are king now and I use them for most of listed purposes. '),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Latest vision models vs florence 2', 'post_score': 2, 'post_id': '1fwo0cr', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwo0cr/latest_vision_models_vs_florence_2/', 'post_author': Redditor(name='RelationshipNeat6468')}, page_content='For captioning and ocr task what exactly are the go to models? So far it seems florence 2 might be the best for captioning but this field is advancing so quickly that something released a month ago may not be the best anymore.\\n\\nSimilarly for ocr what do you recommend is the go to model? \\n\\nSo looking for two types of vision models, florence 2 types for captioning, object detection and segmentation and then for ocr text, tables and diagram extractions.'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Question about \"Best of N Sampling\" method', 'post_score': 3, 'post_id': '1fwm6qc', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwm6qc/question_about_best_of_n_sampling_method/', 'post_author': Redditor(name='noellarkin')}, page_content='I\\'ve been trying out u/asankhs OptiLLM to get a hang of the prompt engineering and reasoning methods.\\n\\nSomething I wondered about the BoN Sampling method - - isn\\'t it true that this depends on the model that\\'s doing the rating knowing what constitutes a good and bad response?\\n\\nWouldn\\'t this necessitate, at the very least, some few shot examples for the particular prompt, or a fine tuned model for a constellation of \"rating\" tasks?\\n\\nWhen using best-of-n approaches, do people use a fine-tuned rating model or just go for the biggest and best model out there?'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Best NSFW story or roleplaying models for 12gb vram?', 'post_score': 111, 'post_id': '1fvy983', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fvy983/best_nsfw_story_or_roleplaying_models_for_12gb/', 'post_author': Redditor(name='Angelica_ludens')}, page_content='Asking for a friend. Better if it can do gay furry vore stuff. '),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Build your real life emotional companion in few dollars', 'post_score': 33, 'post_id': '1fw6gwp', 'post_url': 'https://github.com/StarmoonAI/Starmoon', 'post_author': Redditor(name='FewGate7173')}, page_content='HN link: https://news.ycombinator.com/item?id=41743546'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Llama 3.2 1b . It is useful?', 'post_score': 16, 'post_id': '1fwb6t8', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwb6t8/llama_32_1b_it_is_useful/', 'post_author': Redditor(name='Perfect-Campaign9551')}, page_content=\"I'm able to run this on my phone. It's fun to just mess with prompting but it's definitely NOT very good at following directions in prompts. I was attempting to make it create formatted output and such but it just isn't up to par (yes I know it's very small)\\n\\nWondering, what real usefulness this model is except for maybe just trying little things out?\\n\\n\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Best LLM for code completion? ', 'post_score': 5, 'post_id': '1fwigff', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwigff/best_llm_for_code_completion/', 'post_author': Redditor(name='Not-The-Dark-Lord-7')}, page_content='I recently setup Continue.dev, and the model they advise to use for local autocomplete is Starcoder v2 3b. Is this accurate? As in, is the best small local model for autocomplete? I‚Äôd be willing to go up to 5b or 7b for better results. However, I know these models are specialized, and you can‚Äôt just use any model for autocomplete. So, is this the best local model? '),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'What uncensored LLMs are good for answering questions about hacking and potentially greyarea information?', 'post_score': 26, 'post_id': '1fw4z47', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw4z47/what_uncensored_llms_are_good_for_answering/', 'post_author': Redditor(name='Suitable-Ad-8598')}, page_content='For example: when I ask chatgpt to help me with scraping it will often refuse to do so. How do I unlock the forbidden knowledge? '),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Raspberry Pi 5 vs Orange Pi 5B: Test of LocalLLM Performance', 'post_score': 18, 'post_id': '1fw7jd2', 'post_url': 'https://youtu.be/OXSsrWpIm8o', 'post_author': Redditor(name='Icy-Corgi4757')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'WebLLM Playground - Run Llama 3.2, Qwen, Gemma, etc in the browser on WebGPU', 'post_score': 22, 'post_id': '1fw4vbw', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw4vbw/webllm_playground_run_llama_32_qwen_gemma_etc_in/', 'post_author': Redditor(name='cfahlgren1')}, page_content=\"There's a new playground to run small models on top of WebLLM + WebGPU by MLC.\\n\\nhttps://preview.redd.it/o9d3t3ugyrsd1.png?width=1029&format=png&auto=webp&s=c0fe13fb12562235e9d596701e67f38c6d2dad53\\n\\n[https://huggingface.co/spaces/cfahlgren1/webllm-playground](https://huggingface.co/spaces/cfahlgren1/webllm-playground)\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Why is Transformers Library w/ HuggingFace so slow on Inference compared to Ollama? ', 'post_score': 6, 'post_id': '1fwde4t', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwde4t/why_is_transformers_library_w_huggingface_so_slow/', 'post_author': Redditor(name='BucksinSix2019')}, page_content=\"I have recently been trying to build some fun projects using local LLMs.\\n\\n  \\nI'm having trouble understanding the significant performance gap I'm experiencing between Ollama and the Transformers library when running language models locally.  I started off using Ollama which was incredibly easy to setup and run. I have a GTX 1660 with 6gb VRAM and was running a Q4 model of Llama3.1 just fine with reasonable speeds. I ran this in python using the Ollama library and speeds were still solid. However, I went over to try using Huggingface and even 2B parameter quantized models (Gemma-2-2B-it) are running extremely slow when using the Transformers library in Python.\\n\\nHere's an example of my code using Transformers:\\n\\nhttps://preview.redd.it/6ea4me5nutsd1.png?width=883&format=png&auto=webp&s=00531fd75232c97d88723879472d416a59b3d2bc\\n\\nAs you can see, I'm using standard practices like quantization and setting the device to CUDA. Despite this, the performance is nowhere near what I get with Ollama.\\n\\n1. Is this a common experience, or could there be something specific to my setup causing this issue?\\n2. Are there any optimizations or best practices I'm missing when using Transformers that could significantly improve performance?\\n3. What makes Ollama so much more efficient for local inference compared to Transformers?\\n4. Are there any alternatives to Transformers that offer Ollama-like performance while still providing the flexibility and ecosystem of Hugging Face?\\n\\nI'd greatly appreciate any insights or experiences the community can share. Thanks in advance!\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': \"llm_client: the easiest way to integrate llama.cpp into your Rust project for 'agent' behavior and NLP tasks\", 'post_score': 8, 'post_id': '1fwbcuw', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwbcuw/llm_client_the_easiest_way_to_integrate_llamacpp/', 'post_author': Redditor(name='JShelbyJ')}, page_content='Installable via [crates.io](https://crates.io/crates/llm_client) - automatically builds for windows, linux, mac with or without CUDA.\\n\\nIt\\'s kind of like a Rust Ollama, but the focus is on using LLMs to replace traditional control flow (if statements).\\n\\n\\n    let response: u32 = llm_client.reason().integer()\\n        .instructions()\\n        .set_content(\"Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\")\\n        .return_primitive().await?;\\n\\nThis performs CoT reasoning and returns a number (or boolean or custom string value) you can use in your code. With a small model like phi3.5 and a GPU, it can perform this process in around a second. So, the idea is to use it for agent behavior and NLP tasks.\\n\\nAlso, based on your available VRAM it will estimate the largest quant for the selected model, but you can also specify local models or device configs, or even run multiple models at once.\\n\\nhttps://github.com/shelbyJenkins/llm_client'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'so what happened to the wizard models, actually? was there any closure? did they get legally and academically assassinated? how? because i woke up at 4am thinking about this ', 'post_score': 225, 'post_id': '1fvprgj', 'post_url': 'https://i.redd.it/za3osyhwpnsd1.png', 'post_author': Redditor(name='visionsmemories')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Vocal chat with ollama', 'post_score': 7, 'post_id': '1fwa8dn', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwa8dn/vocal_chat_with_ollama/', 'post_author': Redditor(name='Erimay')}, page_content=\"Hello,\\n\\nHere's my repository for a vocal chat webapp relying upon ollama, llama3.2:1b and ggerganov's whisper.cpp: [https://github.com/erima2020/vocalchat](https://github.com/erima2020/vocalchat)\\n\\nComments are welcome. I do not have much time for this (hardly at all) but I will consider suggestions, if any.\\n\\nBest wishes,\\n\\nEric\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'What is technically \"better\" Q8 or FP8?', 'post_score': 10, 'post_id': '1fw66rm', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw66rm/what_is_technically_better_q8_or_fp8/', 'post_author': Redditor(name='myfavcheesecake')}, page_content='Theoretically which is \"better\"? \\n\\nI understand that the differences between the 2 are minor but out of curiosity works like to know.'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Should I use an agentic framework to build multi-agent infrastructure.', 'post_score': 7, 'post_id': '1fw8dwx', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw8dwx/should_i_use_an_agentic_framework_to_build/', 'post_author': Redditor(name='Zues1400605')}, page_content='I want to build a llm-application, but IDK if I should use something like langchain (or an alternative) vs writing my own AI agents, and tools. What would be better in your opinions? And how should I go about it??\\n\\n'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'A library to \"unmangle\" vocabulary file into actual dict[int, bytes]?', 'post_score': 4, 'post_id': '1fwe2db', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwe2db/a_library_to_unmangle_vocabulary_file_into_actual/', 'post_author': Redditor(name='Huanghe_undefined')}, page_content='From [interesting non-visible byte mangling in GPT2 that is then inherited in huggingface\\'s BBPE preprocessor](https://github.com/openai/gpt-2/blob/master/src/encoder.py#L9), to [Sentencepiece\\'s creative usage of ‚ñÅ(U+2581)](https://github.com/google/sentencepiece), it seems like every tokenizer has its own way to encode/decode its vocabulary. Is there a universal, robust way to \"unmangle\" all these vocabulary file into a simple map from token id to token bytes? Huggingface\\'s tokenizer do not work in this case because they only expose int->str and str->int interface, but an individual token may not constitute a valid UTF-8 string(like <0xF0> in llama2 vocabulary).'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Newbie tinkerer here. If I wanted to build a completely offline MacOS app to summarise text and ask Q&A, what are the steps?', 'post_score': 1, 'post_id': '1fwk1pc', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwk1pc/newbie_tinkerer_here_if_i_wanted_to_build_a/', 'post_author': Redditor(name='Batman_In_Peacetime')}, page_content='At the moment, I am using a CLI command and basic Python scripts to run it (script in first comment). The model I am using is also basic, t5-small. It does work, it can summarise books by chunking the text into 500 tokens and summarising them.\\n\\nI tried using bart-large-cnn but I faced difficulty. Could you please recommend how do I - \\n\\n1. Use a model that does the basic summarization well. I can then build on top of it to add bullet points, etc\\n2. How do I wrap/bundle this into a MacOS app? The input would be text, or PDF file. Output would be text summary. I have been using v0 vercel to make the UI in React. But I\\'m stuck on how to put that React code into a MacOS native app.\\n\\nI have an M1 14\" Pro. I\\'m hoping that is sufficient to run offline processes.\\n\\nThank you so much for your help. I have added the basic Python script I\\'m using in the first comment for you to see.'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Please guide me on what is best for my use case ?', 'post_score': 0, 'post_id': '1fwjm84', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwjm84/please_guide_me_on_what_is_best_for_my_use_case/', 'post_author': Redditor(name='Dazzling-Albatross72')}, page_content='I‚Äôm working on creating a chatbot and have experimented with various RAG setups, but they haven‚Äôt been precise enough for my use case. The chatbot needs to provide accurate answers from a 100-200 page document. Including the entire document in the prompt is too costly, so that approach isn‚Äôt feasible.\\n\\nI‚Äôve also tried fine-tuning the model with question-answer pairs generated from the document, but this hasn‚Äôt worked well. If I ask any question not exactly matching the QA pairs, the model struggles to provide a correct answer.\\n\\nI believe the best solution is to continue pretraining a small LLM like Llama 3.2 or Gemma 2 (2B). If anyone has experience with this, I‚Äôd appreciate any advice or tips. I‚Äôm planning to rent one or two H100 GPUs for up to a week for this purpose.\\n\\nI‚Äôve already tried continued pretraining using the Unsloth notebook, but the results have been unsatisfactory, as they used LoRA and the responses I received were poor. Additionally, I‚Äôm unsure if I should pretrain the base model first and then perform instruction tuning afterward.\\n\\nAnother issue I encountered is that when I continued pretraining Gemma 2 (2B) and then instruction-tuned it, the model forgot the information I added during pretraining. While it still retained its original knowledge, it couldn‚Äôt recall the additional information.\\n\\nAny suggestions or insights would be greatly appreciated!'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Big parameter model quantized 4 bit or small model max precision?', 'post_score': 6, 'post_id': '1fw885k', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw885k/big_parameter_model_quantized_4_bit_or_small/', 'post_author': Redditor(name='Initial_Track6190')}, page_content='\\n\\n[View Poll](https://www.reddit.com/poll/1fw885k)'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Is the Radeon 780M any useful?', 'post_score': 3, 'post_id': '1fwd0nt', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwd0nt/is_the_radeon_780m_any_useful/', 'post_author': Redditor(name='Suitable-Name')}, page_content=\"Hey everyone,\\n\\nI have a new server with an AMD Ryzen 7 PRO 8700GE and 128GB RAM, I'm still setting up at the moment. I didn't really plan to use the server for an LLM, but when I saw the APU, I thought it would be worth asking if it's any useful for running an LLM? Or doesn't it make a big difference compared to pure CPU?\\n\\nThanks for your input!\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Using Weapons as a Metaphor for Different Models', 'post_score': 0, 'post_id': '1fwj4od', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fwj4od/using_weapons_as_a_metaphor_for_different_models/', 'post_author': Redditor(name='flysnowbigbig')}, page_content='**Task 1: Script Debugging**\\n\\n\\n\\nI was reverse-engineering a program and needed to combine static analysis tools with dynamic debugging. Using a script, such as with tools like Frida or Pin, I had to extract the required assembly code and then pull the necessary information to create a data table.\\n\\n\\n\\nClaude: In this kind of task, the overall workflow and thought process were fine, but the details were messy. How simple is this task? It\\'s just a matter of extracting assembly code interspersed between various types of brackets, My attempt to save time ended up wasting time üòÖ. On the other hand, O1 Mini handled it effortlessly.\\n\\n\\n\\n**Task 2: Simplified Version of a Card Game (Big Bang)**\\n\\n\\n\\nThis version had only two skills and around 10 cards. Claude performed well, showing a solid understanding of user interaction. Even though it didn‚Äôt fully understand every prompt, it managed to fix logic errors after a few iterations.\\n\\n\\n\\nHowever, O1 Mini seemed to struggle with concepts like \"turns,\" \"end of turn,\" \"alternating,\" \"discard,\" \"single-use,\" and \"confirm.\" While the implementation was technically correct, there was unnecessary user interaction, making the experience less smooth.\\n\\n\\n\\n**Task 3: Building a Blackjack-like Card Game Server (about 600 lines)**\\n\\n\\n\\nHere, Claude\\'s weaknesses in logical flow became apparent. It struggled with almost every aspect‚Äîdiscarding, betting, raising, and scoring. O1 Mini, due to its limited understanding of real-world concepts, also couldn‚Äôt handle the task well. I tried alternating between the two models, but the result was a mess.\\n\\n\\n\\nWhen I opened a new chat window with Claude, it still insisted that the code was fine, despite the obvious issues üòÖ.\\n\\n\\n\\nFinally, I switched to O1 Preview, which pointed out several errors. After three iterations, the problems were perfectly resolved.\\n\\n\\n\\nAnother test involved giving the server code to the models and asking them to write a bot based on the rules and interaction interface. Claude mostly failed to write A robot that can communicate normally (because it cannot correctly analyze the input and output formats required by the server), but O1 Preview consistently produced bots that actually worked.\\n\\n\\n\\nClaude: Like a handgun, Claude is quick, lightweight, and effective for smaller, simpler tasks. Its ability to handle logical analysis is limited to smaller contexts, even though it can retain longer contexts in a textbook-like memory. Its logical control is weak, and it often fails to predict how code will run in practice. However, it‚Äôs fast and well-suited for small to medium-sized tasks with less complex logic or for simple scripts with common functionality.\\n\\n\\n\\nO1 Mini: Like a bolt-action rifle, O1 Mini excels in precision when it comes to pure symbolic reasoning, such as mathematical or logical puzzles, where real-world concepts are not involved. In this domain, O1 Mini is even stronger than O1 Preview. However, its lack of understanding of real-world concepts often leads to mistakes in business processes or failure to meet practical requirements. It‚Äôs ideal for abstract problem-solving but struggles with tasks that require an understanding of real-world workflows and user interactions.\\n\\n\\n\\nO1 Preview: Like a semi-automatic rifle, O1 Preview strikes a balance between power and efficiency. It‚Äôs capable of handling larger and more complex tasks with better accuracy and consistency, especially when real-world concepts and workflows are involved. While not as precise as O1 Mini in pure symbolic reasoning, O1 Preview excels in understanding real-world tasks and business logic. Its overall efficiency, considering the time spent on thinking and iteration, is at least twice that of Claude, and for complex tasks, it becomes the only viable option, despite its higher cost.'),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Istruction tune of an already istruction tuned LLM', 'post_score': 5, 'post_id': '1fw7o9w', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw7o9w/istruction_tune_of_an_already_istruction_tuned_llm/', 'post_author': Redditor(name='C080')}, page_content=\"Hi I have curated a vast and diverse dataset of high-quality instructions (around 1.5 million pairs) and I'm training to fully fine-tune  (with axolotl) to improve the performance of [https://huggingface.co/iGeniusAI/Italia-9B-Instruct-v0.1](https://huggingface.co/iGeniusAI/Italia-9B-Instruct-v0.1) . \\n\\nNormally I would start with the base model, but in this case, this is not available and the training is not going well after 1 epoch the loss is low (around 0.6-0.8) but the model is performing worse on a set of eval (like Italian mmlu etc...) I'm testing the checkpoint with\\n\\n  \\nDoes any of you have any intuition about this? Does too much instruction tuning worsen the model? Should I use my dataset as continuous pretraining for a bit and then instruction tune it?\\n\\n  \\nThanks for the pointers!\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Gemma 2 2b-it is an underrated SLM GOAT', 'post_score': 105, 'post_id': '1fvowgm', 'post_url': 'https://i.redd.it/18x465phhnsd1.png', 'post_author': Redditor(name='TitoxDboss')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Gentle continued lighthearted prodding. Love these devs. We‚Äôre all rooting for you! ', 'post_score': 391, 'post_id': '1fvf7fx', 'post_url': 'https://i.redd.it/uhqultuj8lsd1.jpeg', 'post_author': Redditor(name='Porespellar')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Nemo minitron instruct', 'post_score': 4, 'post_id': '1fw7rqe', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw7rqe/nemo_minitron_instruct/', 'post_author': Redditor(name='kind_cavendish')}, page_content=\"Nemo minitron instruct is out! https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct#:~:text=Minitron%2D8B%2DInstruct-,Model%20Overview,augmented%20generation%2C%20and%20function%20calling.\\nHaven't seen anyone really talking about it but I thought it was good, even though I was using the wrong prompt template.\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Use 1b to 3b models to classify text like BERT?', 'post_score': 21, 'post_id': '1fvvqua', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fvvqua/use_1b_to_3b_models_to_classify_text_like_bert/', 'post_author': Redditor(name='Elegant_Fold_7809')}, page_content=\"Was anyone able to use the smaller models and achieve the same level of accuracy for text classification with BERT? I'm curious if the encoder and decoder can be separated for these llms and then use that to classify text.\\n\\nAlso is BERT/DEBERTA still the go to models for classification or have they been replaced by newer models like BART by facebook?\\n\\nThanks in advance\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Target hardware questions... Vram suggestions.', 'post_score': 8, 'post_id': '1fw2jtz', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fw2jtz/target_hardware_questions_vram_suggestions/', 'post_author': Redditor(name='killjoyparris')}, page_content=\"For the past month or so, I've been playing around with local llms, on an 8gb GTX1080. I've been thinking about upgrading my graphics card, but I also don't want to spend a bunch of money. LoL. I know there are a bunch of posts like this, but I'm still not quite sure what the most cost effective option is... If possible, I'd like to be able to run quantized 70B parameter models at at least 5-8 t/s.\\n\\nMost of the posts I've read suggest either going the dual 3090/4090 or M40/P40 route. I feel like the cost of buying two 3090's, 4090's, and even two P40's is more than I feel comfortable spending right now. And, I feel like the performance of the M40 might be a little too slow for real time use with larger models and bigger context.\\n\\n \\n\\nSo,... I'm kinda looking for suggestions on other options.\\n\\nOne option I've been weighing is possibly purchasing 2 midrange 16 gb cards. Things like the Intel A770 and RX 6700 XT seem have decent benchmarks, and a price point of around $300. Another option I've been weighing is purchasing a single 3090 and running it along side my GTX 1080 for a total of 32 gb of vram. And, trying to find a deal on a 16 GB card to update my GTX and purchasing a 3090 for total of 40 gb of vram is also an option.\\n\\n\\n\\nHere is sort of of more organized layout of what I've been looking at:\\n\\n|Card|Vram|Estimated Cost|\\n|:-|:-|:-|\\n|2 M40's|48 GB|$300|\\n|2 midrange 16-gb cards|32 GB|$600|\\n|upgrade to single 3090|32 GB|$700|\\n|1 16-gb card & 1 3090|40 GB|$1,000|\\n|2 P40's|48 GB|$900|\\n|2 3090's|48 GB|$1,400|\\n\\n  \\nRight now, the most attractive option to me is kinda looking like the cheapest one using modern gpus, with the 2 16-gb cards. Does anyone have any insight into what performance any of the above set ups might look like? In particular, does anyone have any insight into oobabooga with the Intel A770 or RX 6700 XT, or what performance looks like? Does anyone know if 2 of those 16-gb cards together will be able to run a 70B parameter at Q2 model at 5-8 t/s?\\n\\nDoes anyone have any advice or insight to lend about total vram targets? I'm upgrading from a single 8gb card, so I'm pretty sure more vram is generally is the way to go. But, are there any major features or models that I wouldn't be able to run 32 GB, vs 40 GB, or 48 GB? Is there a big difference in 32 GB vs 48 GB, that I should take into account?\"),\n",
       " Document(metadata={'post_subreddit': 'r/LocalLLaMA', 'post_category': 'hot', 'post_title': 'Higher capacity regular DDR5 timeline? 64GBx2 96GBx2?', 'post_score': 17, 'post_id': '1fvw9sd', 'post_url': 'https://www.reddit.com/r/LocalLLaMA/comments/1fvw9sd/higher_capacity_regular_ddr5_timeline_64gbx2/', 'post_author': Redditor(name='capybooya')}, page_content=\"I'm struggling with my Google skills on this one, I seem to remember reading in the last year or so that higher density DDR5 would arrive soon. And for those of us running these models on regular desktop PC's, we want the maximum memory capacity in 2 DDR5 sticks for the minimum hassle. Does anyone know if there are higher capacity sticks and kits on the horizon anytime soon? We have had the choice of 2x48GB (96GB) for a while, and I'd hope to see 2x64GB or 2x96GB be available soon.\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Running LLMs Locally', 'post_score': 86, 'post_id': '151w6jf', 'post_url': 'https://www.reddit.com/r/LLM/comments/151w6jf/running_llms_locally/', 'post_author': Redditor(name='Eaton_17')}, page_content='I‚Äôm new to the LLM space, I wanted to download a LLM such as Orca Mini or Falcon 7b to my MacBook locally. I am a bit confused at what system requirements need to be satisfied for these LLMs to run smoothly. \\n\\nAre there any models that work well that could run on a 2015 MacBook Pro with 8GB of RAM or would I need to upgrade my  system ?\\n\\nMacBook Pro 2015 system specifications:\\n\\nProcessor: 2.7 GHZ dual-core i5\\nMemory: 8GB 1867 MHz DDR 3\\nGraphics: intel Iris Graphics 6100 1536 MB.\\n\\nIf this is unrealistic, would it maybe be possible to run an LLM on a M2 MacBook Air or Pro ? \\n\\nSorry if these questions seem stupid.'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Decoding the preprocessing methods in the pipeline of building LLMs', 'post_score': 14, 'post_id': '151x1pz', 'post_url': 'https://www.reddit.com/r/LLM/comments/151x1pz/decoding_the_preprocessing_methods_in_the/', 'post_author': Redditor(name='moribaba10')}, page_content='1. Is there a standard method for tokenization and embedding? What tokenization methods are used by top LLMs like GPT version and bard etc? \\n2. In the breakdown of computation required for training LLMs and running the models which method/task takes the most amount of computation unit? '),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': \"There's an Azure OpenAI sub for which a primary key and a secondary key exists which is managed via the Azure portal. Current rate limits applies to these keys. There's option to regenerate keys via the portal. How to have multiple keys within the same subscription, with separate rate limits?\", 'post_score': 4, 'post_id': '15163i5', 'post_url': 'https://www.reddit.com/r/LLM/comments/15163i5/theres_an_azure_openai_sub_for_which_a_primary/', 'post_author': Redditor(name='fofxy')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Does USA have free online legal databases in the same format as BAILII, CanLII, CommonLII, etc?', 'post_score': 8, 'post_id': '150q4d1', 'post_url': 'https://law.stackexchange.com/q/93639/41', 'post_author': Redditor(name='0scot')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Jobs with a LLM', 'post_score': 16, 'post_id': '14zoduh', 'post_url': 'https://www.reddit.com/r/LLM/comments/14zoduh/jobs_with_a_llm/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content='Hello everyone \\nI wanna know what jobs are available for people who only have a LLM degree? \\nCan u work as a paralegal with only a LLM ? \\nI know finding a job with LLM degree is not easy but ANY job until it‚Äôs related to law and legal matter is alright,so if you have any information,id appreciate itü§ç'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'How do you Monitor Your Production LLM based Application?', 'post_score': 4, 'post_id': '14za6sw', 'post_url': 'https://www.reddit.com/r/LLM/comments/14za6sw/how_do_you_monitor_your_production_llm_based/', 'post_author': Redditor(name='magiklabsio')}, page_content=\"  \\nIf anyone is struggling with hallucinations, testing / monitoring or improving accuracy of their LLM based apps, we've been working on a solution that we're launching this week.Send me a DM - would love to chat and see if we can help.\\n\\nwww.magiklabs.app\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': \"Hey folks! Ever wished you could get a mind-blowing brainstorm report generated by AI agents in just 20 minutes? Well, guess what? You heard it right! I'm thrilled to introduce my very first web app, BrainstormGPT!\", 'post_score': 7, 'post_id': '14yiby1', 'post_url': 'https://www.reddit.com/r/LLM/comments/14yiby1/hey_folks_ever_wished_you_could_get_a_mindblowing/', 'post_author': Redditor(name='BrainstormGPT')}, page_content=' Now, you might be wondering, \"What on earth is BrainstormGPT?\" ü§î Well, my friends, it\\'s not just another search engine that makes you yell \"Yahoo!\" This is a cool application where you simply enter a \"topic,\" and voila! In a short 20-minute span, AI-powered agents will discuss automaticly and  generate a jaw-dropping \"Brainstorm Report\" for you! Sounds intriguing, doesn\\'t it? üòé  \\nImagine having an important meeting, paper, or a creative task that requires your brain to be in turbo mode, but you\\'re stuck in an \"inspiration desert.\" Don\\'t fret, BrainstormGPT is here to save the day! It will be your ultimate sidekick, using its unique AI magic to transport you to a realm of boundless inspiration!\\n\\nNow, click here [https://brainstormgpt.ai/](https://brainstormgpt.ai/) to check out this web app I\\'ve proudly built! Whether you\\'re into AI or simply seeking that spark of creativity, I guarantee this app will blow your mind! üí•'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Best way to map user questions to code functions', 'post_score': 2, 'post_id': '14x9i7e', 'post_url': 'https://www.reddit.com/r/LLM/comments/14x9i7e/best_way_to_map_user_questions_to_code_functions/', 'post_author': Redditor(name='rzepeda1')}, page_content='Hi ! I‚Äôm working on a integration of chat gpt with a 3D model viewer . The idea would be that a user will ask questions for example ‚Äú show me room 102‚Äù and that space will isolate in the viewer . \\n\\nThe way I have it sent up I think is not robust which is I set up a initial promp that basically tells the model to respond with an array of ids if the user mention the words ‚Äú show me ‚Äú and then trigger a function if that condition is meet on the front end \\n\\nAny ideas are welcome üôè'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Fine-Tuning Insights: Lessons from Experimenting with RedPajama Large Language Model on Flyte Slack Data', 'post_score': 5, 'post_id': '14vu4dk', 'post_url': 'https://www.union.ai/blog-post/fine-tuning-insights-lessons-from-experimenting-with-redpajama-large-language-model-on-flyte-slack-data', 'post_author': Redditor(name='allasamhita')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Falcon 40B - impressive local LLM', 'post_score': 2, 'post_id': '14vuy2k', 'post_url': 'https://www.youtube.com/watch?v=-IV1NTGy6Mg', 'post_author': Redditor(name='TheAliveIndicator')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Are \"Language Models\" simply Decoder-Only Transformers?', 'post_score': 3, 'post_id': '14vm80u', 'post_url': 'https://www.reddit.com/r/LLM/comments/14vm80u/are_language_models_simply_decoderonly/', 'post_author': None}, page_content='I\\'ve read many papers where authors specify the phrase \"language model\". Now I know it is specific to each paper, but is it mostly referred to decoder-only transformers? Consider the following excerpt from the BART paper - \\n\\n\"BART is trained by corrupting documents and then optimizing a reconstruction loss‚Äîthe cross-entropy between the decoder‚Äôs output and the original document. Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption. In the extreme case, where all information about the source is lost, **BART is equivalent to a language model.**\" What does \"language model\" exactly mean here? '),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'AI player companions?', 'post_score': 2, 'post_id': '14vl5bz', 'post_url': 'https://www.reddit.com/r/LLM/comments/14vl5bz/ai_player_companions/', 'post_author': Redditor(name='Bells_Theorem')}, page_content='Interested in thoughts on LLMs functioning as player companions in video games? How likely and how soon do you think this will be a thing?\\n\\nThis is where we are now: [https://www.youtube.com/watch?v=AQq8M88s3BU&t=45s](https://www.youtube.com/watch?v=AQq8M88s3BU&t=45s)'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': \"Introduction to Language Models (LLM's, Prompt Engineering, Encoder/Deco...\", 'post_score': 4, 'post_id': '14ux581', 'post_url': 'https://youtube.com/watch?v=9PGmMdkTZls&feature=share', 'post_author': Redditor(name='Neurosymbolic')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Is there decent open-source LLMs faster than Falcon-7b-instruct?', 'post_score': 2, 'post_id': '14tx6ou', 'post_url': 'https://www.reddit.com/r/LLM/comments/14tx6ou/is_there_decent_opensource_llms_faster_than/', 'post_author': Redditor(name='daanmolen')}, page_content='Hi,\\n\\nI recently tried Falcon-40b and Falcon-7b-instruct (locally).\\n\\nI only setup Falcon-7b-instruct on my local computer due to RAM limitations (32 GB)\\n\\nI should say it is better than I expected but way to slow in comparison to OpenAI API, understandably.\\n\\n&#x200B;\\n\\nAs far as I can see, they rank open LLMs based on their linguistic/rationalistic performance but not according to speed.\\n\\n[Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) \\n\\nSo here is my question:\\n\\n**Is there any open source LLMs which has decent performance but not as slow as Falcon-7b-instruct?**\\n\\n&#x200B;\\n\\nThanks for sharing your experiences in advance.'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'General LLM VS LLM with concentration,which one is better for future career ?', 'post_score': 10, 'post_id': '14t35gc', 'post_url': 'https://www.reddit.com/r/LLM/comments/14t35gc/general_llm_vs_llm_with_concentrationwhich_one_is/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content=\"Guys, I have a question \\nIn regard both academia and practicing law, which one is better? \\nAn LLM with specific concentration or a general LLM \\n\\nI‚Äôm not sure if I wanna practice law or stay in academia, which is why I wanna know which kind of LLM will be more helpful to me if I choose to stick with academia \\n\\nAnd also in regard to law firms and big law in general, what I have heard is that they don't really care about LLM and SJD and the only thing that matter to them is JD\\nis it true? I mean won't u get a higher salary if you have an LLM or SJD?\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'NSQL: First Ever Fully OpenSource SQL Foundation Model', 'post_score': 2, 'post_id': '14sy3n9', 'post_url': 'https://ithinkbot.com/nsql-first-ever-fully-opensource-sql-foundation-model-f7b501d91ca4', 'post_author': Redditor(name='Ok-Range1608')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Book recommendation please', 'post_score': 2, 'post_id': '14swvwp', 'post_url': 'https://www.reddit.com/r/LLM/comments/14swvwp/book_recommendation_please/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content='Hi everyone \\nI‚Äôm an international law student (LLM) at PSU and i wonder what are the best books on civil procedure,criminal law,professional responsibility and legal writing and research? \\nI‚Äôm aware they are so many books explaining these courses but i don‚Äôt know what book is the best,thats why i‚Äôm writing this post \\n\\nI‚Äôm looking for books that has a clear writing style and and also can help with exams and tests \\n\\nI appreciate any recommendation,thank you so much‚ù§Ô∏è'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'What GPA did you get in your LLM program?', 'post_score': 4, 'post_id': '14s9vhr', 'post_url': 'https://www.reddit.com/r/LLM/comments/14s9vhr/what_gpa_did_you_get_in_your_llm_program/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content=\"Hello everyone \\nI'm an incoming LLM student and I'm anxious about classes and grades \\nI wanna know how hard is it to get a high GPA ?\\nAnd how hard are classes?\\nAlso did you guys get to publish an article with supervision of your professors or by yourself in your LLM period? \\n\\nAny comment is highly appreciated ü§ç\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Who chooses what tools you can use for work?', 'post_score': 0, 'post_id': '14sfzz4', 'post_url': 'https://www.reddit.com/r/LLM/comments/14sfzz4/who_chooses_what_tools_you_can_use_for_work/', 'post_author': Redditor(name='Kindly_Job_5126')}, page_content=\"Question for engineers and data scientists doing work that involves creating AI solutions, especially generative AI for text.\\n\\n**1) Who at your company is the key purchasing decision maker on what tooling you can use to do your work?**  \\n*For example: Vice President of Engineering for the AI pillar*\\n\\n**2) How big is your company (roughly how many employees)?**\\n\\n**3) What is your biggest technical barrier with making chat GPT work for your use case?** \\n\\n*For example: Gathering data examples for fine tuning*\\n\\n  \\nThat's all! Thanks in advance!\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Seeking Advice: Building Language Models for Non-English Languages (e.g., Spanish or Japanese)', 'post_score': 0, 'post_id': '14rdyhh', 'post_url': 'https://www.reddit.com/r/LLM/comments/14rdyhh/seeking_advice_building_language_models_for/', 'post_author': Redditor(name='jolly1404')}, page_content='Hello fellow Redditors,\\n\\nI am currently working on a project with the goal of building Language Models (LLMs) that can understand and process non-English languages, specifically focusing on languages such as Spanish or Japanese. I am seeking advice and guidance on how to effectively accomplish this task, including continuous testing and benchmarking.\\n\\nMy aim is to develop LLMs that can comprehend and generate text in languages other than English, allowing for more inclusive and comprehensive language processing capabilities. By achieving this, we can enhance communication and language understanding for speakers of various languages worldwide.\\n\\nHere are a few specific questions I have:\\n\\n1. **Data Collection:** What are the recommended approaches for collecting large amounts of text data in languages like Spanish or Japanese? Are there any publicly available datasets or resources that I should consider utilizing?\\n2. **Training and Fine-tuning:** Once I have gathered the data, what are the best practices for training and fine-tuning language models for non-English languages? Are there any specific considerations or techniques that differ from training English language models?\\n3. **Evaluation Metrics:** How can I evaluate the performance and quality of the non-English LLMs? Are there any established evaluation metrics or benchmarks for assessing the accuracy and fluency of text generation in languages other than English?\\n4. **Continuous Testing and Benchmarks:** What are the recommended approaches for continuously testing and benchmarking non-English language models? Are there any ongoing projects or platforms that provide resources or standardized evaluation suites for non-English languages?\\n5. **Language-Specific Challenges:** Are there any unique challenges or complexities associated with building LLMs for languages like Spanish or Japanese? What are the potential obstacles I should be prepared for during the development process?\\n6. **Community Collaboration:** Are there existing communities or forums where researchers or developers working on non-English language models gather to collaborate and share knowledge? I would appreciate any recommendations for engaging with like-minded individuals or groups.\\n\\nIf you have any insights, experiences, or suggestions regarding any of these aspects, including continuous testing and benchmarking, I would greatly appreciate your input. Building Language Models for non-English languages is an exciting and important endeavor, and I am eager to make progress in this area.\\n\\nThank you all in advance for your time and expertise!\\n\\n**Note:** If you know any other subreddits or online communities where I could cross-post this question for more visibility and responses, please let me know.'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'How many credits should I take?', 'post_score': 3, 'post_id': '14qisk5', 'post_url': 'https://www.reddit.com/r/LLM/comments/14qisk5/how_many_credits_should_i_take/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content='Hi \\nHope you are doing great \\nDo you think 14 credit is too much and I should only take 12credit and focus on my grades? Or you think 14 credit is still manageable?'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'LLM IN UK', 'post_score': 3, 'post_id': '14ptcgv', 'post_url': 'https://www.reddit.com/r/LLM/comments/14ptcgv/llm_in_uk/', 'post_author': Redditor(name='Fun_Palpitation_680')}, page_content='Hellooo im studying LLB uni of london programme (distance learning) and im a pakistani student. I plan on applying to Warwick next year for LLM, but i cant afford the tuition fees. Can someone tell me about the available scholarships and possible fee waivers. I Have scored Merits in the modules, but no disitnction. Also if there are any other cheap options for LLM then do lmkk plss and thanku'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Best Law books', 'post_score': 1, 'post_id': '14pvcgq', 'post_url': 'https://www.reddit.com/r/LLM/comments/14pvcgq/best_law_books/', 'post_author': Redditor(name='Sunnygirlishere')}, page_content='Hi everyone \\nI‚Äôm an international law student in USA and i wonder what are the best books on civil procedure,criminal law,professional responsibility and legal writing and research? \\nI‚Äôm aware they are so many books explaining these courses but i don‚Äôt know what book is the best,thats why i‚Äôm writing this post \\n\\nI‚Äôm looking for books that has a clear writing style and and also can help with exams and tests \\n\\nI appreciate any recommendation,thank you so much‚ù§Ô∏è'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'customizing llm with subreddit data.', 'post_score': 0, 'post_id': '14p2wbg', 'post_url': 'https://www.reddit.com/r/LLM/comments/14p2wbg/customizing_llm_with_subreddit_data/', 'post_author': Redditor(name='Mbuguah_Brian')}, page_content='I am try to create my own llm program with langchain, I wANT TO USE THE REDDIT DATA IN a some subreddits. what can i use i am absolutely new at this so dont assume i know some conc. technicalities. plus i want my llm to hav OCR.capabailites. '),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'A Blueprint for AI Regulation', 'post_score': 0, 'post_id': '14mjgey', 'post_url': 'https://www.reddit.com/r/LLM/comments/14mjgey/a_blueprint_for_ai_regulation/', 'post_author': Redditor(name='theaarushgupta')}, page_content='[https://aarushgupta.com/2023/06/29/regulation-blueprint](https://aarushgupta.com/2023/06/29/regulation-blueprint.html)'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Powerpoint generator', 'post_score': 0, 'post_id': '14maqvx', 'post_url': 'https://www.reddit.com/r/LLM/comments/14maqvx/powerpoint_generator/', 'post_author': Redditor(name='manueljishi')}, page_content=\"Hello there!\\n\\nI'm writing because I have this large writen document that I want to convert to a powerpoint so I was wondering if there are any tools that can provide some help along those lines. Maybe not convert the whole text to presentations but create some structure or some guidance.\\n\\nThank you very much\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Tax LLM advice', 'post_score': 2, 'post_id': '14lt09k', 'post_url': 'https://www.reddit.com/r/LLM/comments/14lt09k/tax_llm_advice/', 'post_author': None}, page_content='I‚Äôm a new-ish lawyer licensed to practice outside the US. Been thinking about a Tax LLM, specializing in transfer pricing, since I work for one of the big 4 accounting firms here in our jurisdiction. My grades in law school were average/barely hitting above average but the field I‚Äôm in right now is very interesting to me. How screwed am I with the application process?'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'LLM Comparison', 'post_score': 0, 'post_id': '14kr27u', 'post_url': 'https://www.reddit.com/r/LLM/comments/14kr27u/llm_comparison/', 'post_author': Redditor(name='Amazing-Cucumber-207')}, page_content='[https://evolusion.ai/](https://evolusion.ai/)'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Fine tuning Openllama', 'post_score': 0, 'post_id': '14kcvza', 'post_url': 'https://www.reddit.com/r/LLM/comments/14kcvza/fine_tuning_openllama/', 'post_author': Redditor(name='mathageche')}, page_content='I am trying to fine tune opnellama 7b for text generation. It gives very bad outputs. Is there anyway to improve it ? -\\\\_-'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Running LLM As Chatbot in your cloud (AWS/GCP/Azure) with a single command', 'post_score': 0, 'post_id': '14k620z', 'post_url': 'https://github.com/dstackai/LLM-As-Chatbot/wiki/Running-LLM-As-Chatbot-in-your-cloud', 'post_author': Redditor(name='cheptsov')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'INTRODUCING SECONDBRAIN: AN OPEN-SOURCE WEB APP THAT SERVES AS AN ALTERNATIVE TO OPENAI. WITH THE ABILITY TO ADD CUSTOM KNOWLEDGE USING PDF, SOURCE LINKS, AND WIKIPEDIA, SECONDBRAIN OFFERS PERSONALIZED ASSISTANCE ALONGSIDE INTELLIGENT RESPONSES, MAKING IT AN INVALUABLE TOOL FOR RESEARCH, WRITING etc', 'post_score': 2, 'post_id': '14jlssw', 'post_url': 'https://v.redd.it/i1x8ryto1e8b1', 'post_author': Redditor(name='everydaycodings')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Two mini-courses for chatting with Github and Arxiv using LangChainAI, OpenAI, ChromaDB, Pinecone and ArizeAI', 'post_score': 2, 'post_id': '14hq06m', 'post_url': '/r/LLMDevs/comments/14hpzj7/two_minicourses_for_chatting_with_github_and/', 'post_author': Redditor(name='Optimal-Resist-5416')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Use LLMs for scam detection', 'post_score': 0, 'post_id': '14hmu11', 'post_url': 'https://www.reddit.com/r/LLM/comments/14hmu11/use_llms_for_scam_detection/', 'post_author': Redditor(name='Top_Career_2354')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'AI-based content recommendation', 'post_score': 0, 'post_id': '14gdddo', 'post_url': 'https://www.reddit.com/r/LLM/comments/14gdddo/aibased_content_recommendation/', 'post_author': Redditor(name='yjpt')}, page_content='Would anyone be interested in trying out a new content recommendation app? Used some AI for personalization and summarization. Feel free to shit on me. Love you all.\\n\\nhttps://apps.apple.com/us/app/pagetok-ai-for-you/id1575852988'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'What is the approach for extraction of structured data from financial documents', 'post_score': 0, 'post_id': '14g8nie', 'post_url': '/r/LocalLLaMA/comments/14g691g/what_is_the_approach_for_extraction_of_structured/', 'post_author': Redditor(name='SisyphusRebel')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'LLMs', 'post_score': 0, 'post_id': '14fzfjj', 'post_url': 'https://www.reddit.com/r/LLM/comments/14fzfjj/llms/', 'post_author': Redditor(name='ABdulBAsit00k')}, page_content=\"I'm looking for resources to learn about the most advanced LLMs right now for different task and where can I get the news about the ongoing development in LLMs or Generative AI.\\n\\n&#x200B;\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'ChatGPT-based Research Pilot', 'post_score': 0, 'post_id': '14fls5p', 'post_url': 'https://researchpilot.fly.dev', 'post_author': Redditor(name='Numerous_Week_436')}, page_content='Hey guys, I‚Äôve been working on a research tool that provides information and analysis on recent events. I wasn‚Äôt impressed with what was currently available so I developed one myself.\\n\\nHere‚Äôs the site: https://researchpilot.fly.dev\\n\\nI based the architecture on this paper:\\nhttps://arxiv.org/abs/2212.10496\\n\\nIt‚Äôs free to use and doesn‚Äôt require a user account. I hope it‚Äôs useful, and I‚Äôm still adding features and capabilities.\\n\\nI‚Äôd love to hear feedback if you guys use it!'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'I got a llm to make me a snake gAME!!!', 'post_score': 0, 'post_id': '14f1lf9', 'post_url': 'https://www.reddit.com/r/LLM/comments/14f1lf9/i_got_a_llm_to_make_me_a_snake_game/', 'post_author': Redditor(name='Toaster496')}, page_content='[https://chat.openai.com/share/642b2837-5513-4ab5-a4e1-bae48ba22a46](https://chat.openai.com/share/642b2837-5513-4ab5-a4e1-bae48ba22a46)'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Ramping up on LLMs', 'post_score': 0, 'post_id': '14d53io', 'post_url': 'https://www.reddit.com/r/LLM/comments/14d53io/ramping_up_on_llms/', 'post_author': Redditor(name='sidbond86')}, page_content='Hi fellow redditors!!\\n\\nI am interested in ramping up on Large Language Models. I am primarily a distributed systems engineer. And my first goal is to get a lay of the land of this technology space and then get some hands-on coding experience with LLMs. My current knowledge is non-existent - like being able to ask basic questions and followups to chatgpt.\\n\\nWhat resources would you recommend that I should review. Looking for outside-in learning. Open to reading articles / tutorials / courses.'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Low VRAM users - best model', 'post_score': 0, 'post_id': '14cuzxz', 'post_url': 'https://www.reddit.com/r/LLM/comments/14cuzxz/low_vram_users_best_model/', 'post_author': Redditor(name='OkHelicopter26')}, page_content='What is the best way to generate text when I only have 6gb video card? What is the best gui option, what is the best command line option?'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'google course on generative ai', 'post_score': 0, 'post_id': '14ceg8n', 'post_url': 'https://youtube.com/watch?v=1wXnYyLzjhI&feature=share', 'post_author': Redditor(name='developer_how_do_i')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Automate any task with a single AI command (Open Source)', 'post_score': 0, 'post_id': '14bhtp1', 'post_url': 'https://www.reddit.com/r/LLM/comments/14bhtp1/automate_any_task_with_a_single_ai_command_open/', 'post_author': Redditor(name='WolfPossible5371')}, page_content=\"Hi everyone!\\n\\nIn the LLM Community, there is a growing trend of utilizing high-powered models like GPT-4 for building platforms that tackle complex tasks. However, this approach is neither cost-effective nor feasible for many open-source community developers due to the associated expenses, inaccuracies, privacy concerns. In response, Nuggt emerges as an open-source project aiming to provide a platform for deploying agents to solve intricate tasks while relying on smaller and less resource-intensive LLMs. We strive to make task automation accessible, affordable, and secure for all developers in the community!\\n\\nüîó **Find Nuggt on GitHub:** [**Nuggt Github Repository**](https://github.com/Nuggt-dev/Nuggt)\\n\\n[Snake Game Demo](https://reddit.com/link/14bhtp1/video/ehyuimozei6b1/player)\\n\\nWhile our current implementation leverages the power of GPT-3.5 (already a huge reduction from the GPT-4 alternative), we recognize the need for cost-effective solutions without compromising functionality. Our ongoing efforts involve exploring and harnessing the potential of smaller models like Orca and Vicuna 13B, ensuring that task automation remains accessible to a wider audience.\\n\\nüîé **Call for Feedback**: We invite the community to try out Nuggt and provide valuable feedback. Let us know your thoughts, suggestions, and any improvements you'd like to see. Your feedback will help us shape the future of Nuggt and make it something valuable to the masses.\\n\\nüí° **Contributors Wanted**: We believe in the power of collaboration! If you're passionate about automation, AI, or open-source development, we welcome your contributions to Nuggt. Whether it's code improvements, new features, or documentation enhancements, your contributions will make a difference.\\n\\nüåü **Join the Nuggt Community:** Get involved, contribute, and join the discussions on our [**Github repository**](https://github.com/Nuggt-dev/Nuggt). We're building a vibrant community, and we'd love to have you on board! \\n\\nTwitter: [https://twitter.com/OfficialNuggt](https://twitter.com/OfficialNuggt)\\n\\nDiscord Server: [https://discord.gg/y7W2zGn9](https://discord.gg/y7W2zGn9)\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'If you have an LLM from the UK can you take the bar in the states (D.C) and practice?', 'post_score': 1, 'post_id': '14abze7', 'post_url': 'https://www.reddit.com/r/LLM/comments/14abze7/if_you_have_an_llm_from_the_uk_can_you_take_the/', 'post_author': Redditor(name='HopeisnearGodislove')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'LLM', 'post_score': 0, 'post_id': '14a6952', 'post_url': 'https://www.reddit.com/r/LLM/comments/14a6952/llm/', 'post_author': Redditor(name='hamza_laaich')}, page_content='hey guys , i need to build a large language model that generates so data analytics presentations for business besides text ... and this is my first experience with LLMs ,  how should i start ? any suggestions ? advices ??'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Extracting mathy text from pdf', 'post_score': 0, 'post_id': '149h00c', 'post_url': 'https://www.reddit.com/r/LLM/comments/149h00c/extracting_mathy_text_from_pdf/', 'post_author': Redditor(name='Longjumping_Essay498')}, page_content=\"How can I use python to extract the maths textbooks from pdf? Pdfminer doesn't retain formatting. Thanks.\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'LLM who is going to take the NY bar exam needs help.', 'post_score': 5, 'post_id': '148n3r5', 'post_url': 'https://www.reddit.com/r/LLM/comments/148n3r5/llm_who_is_going_to_take_the_ny_bar_exam_needs/', 'post_author': Redditor(name='Financial-Map-6625')}, page_content=\"Hello friends, I'm a foreign lawyer who is going to take tge next NY bar exam (not the one next July) and I need general help from any self-study LLM who took the bar what are the books did you use for that. I just want to say that lectures and courses are not for me. Also, where can I find a book with previous NY bar exams \\nany tip or advice will be much appreciated\"),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Shorts Company', 'post_score': 0, 'post_id': '1482rk4', 'post_url': 'https://www.reddit.com/r/LLM/comments/1482rk4/shorts_company/', 'post_author': Redditor(name='iconicdark')}, page_content='We are a small shorts-making company dedicated to helping long-form video creators expand their reach and engage with their audience through captivating short-form videos tailored for platforms like TikTok, Instagram, and Twitter. Our mission is to assist content creators in maximizing their online presence and attracting more followers. Discover how our services can elevate your social media game today!\\n\\nTiktok: https://www.tiktok.com/@shortscompanysc?lang=en\\n\\nInstagram: https://www.instagram.com/shortscompanysc/\\n\\nYoutube: https://www.youtube.com/channel/UCpJTQOF4Y20NF8o0K9c2JVg'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Hello everyone! I have Andrew NG‚Äôs ML specializatin course and I‚Äôve been working. I want to work for LLMs. What should I do? Can you suggest roadmap for me? Thank you', 'post_score': 0, 'post_id': '147v0lx', 'post_url': 'https://www.reddit.com/r/LLM/comments/147v0lx/hello_everyone_i_have_andrew_ngs_ml_specializatin/', 'post_author': Redditor(name='KaiKawaii0')}, page_content=''),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': \"For anyone that's interest. Here's a video I made about a voice llm modeled to tell you your horoscope. Number to call and try is in the description!\", 'post_score': 0, 'post_id': '143kwmn', 'post_url': 'https://www.reddit.com/r/LLM/comments/143kwmn/for_anyone_thats_interest_heres_a_video_i_made/', 'post_author': Redditor(name='inattentionisalluned')}, page_content='https://youtube.com/watch?v=jniofUxkqQs&feature=share9'),\n",
       " Document(metadata={'post_subreddit': 'r/LLM', 'post_category': 'hot', 'post_title': 'Replace UI with chat', 'post_score': 0, 'post_id': '143iakd', 'post_url': 'https://www.reddit.com/r/LLM/comments/143iakd/replace_ui_with_chat/', 'post_author': Redditor(name='ole72444')}, page_content=\"How can one replace the UI of an application with an LLM's chat window? The bot should be able to do everything it used to but via natural language. So the end user doesn't have to click at buttons or view options in a menu; rather, he/she should be able to tell this via simple sentences, which can trigger the usual APIs that were event (click/hover) driven. Are there any existing projects in github or a definite approach to solving this?\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import RedditPostsLoader\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# load using 'subreddit' mode\n",
    "loader = RedditPostsLoader(\n",
    "    client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "    user_agent=\"extractor by u/tmdqja75\",\n",
    "    categories=[\"hot\"],  # List of categories to load posts from\n",
    "    mode=\"subreddit\",\n",
    "    search_queries=[\n",
    "        \"LocalLLaMA\",\n",
    "        \"LLM\"\n",
    "    ],  # List of subreddits to load posts from\n",
    "    number_posts=50,  # Default value is 10\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    \"post_subreddit\",\n",
    "    \"post_category\",\n",
    "    \"post_title\",\n",
    "    \"post_score\",\n",
    "    \"post_id\",\n",
    "    \"post_url\",\n",
    "    \"post_author\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google has developed a system that enables an LLM to undergo \"edit cycles\" before generating a final response. o1 but for creative writing in notebookLM?\n",
      "https://v.redd.it/bc9o56epuusd1\n",
      "\n",
      "Open sourcing Grok 2 with the release of Grok 3, just like we did with Grok 1!\n",
      "https://x.com/elonmusk/status/1842248588149117013\n",
      "\n",
      "Slightly overdid the question revision step, I think\n",
      "https://i.redd.it/7wii9vj8nwsd1.png\n",
      "\n",
      "60GB VRAM Ziptie Build for 2400‚Ç¨\n",
      "https://www.reddit.com/gallery/1fwcu30\n",
      "\n",
      "I tested few TTS apps ‚Äì You can decide what's the best\n",
      "https://v.redd.it/c7daserjxwsd1\n",
      "\n",
      "<Looks at watch> ü§®\n",
      "https://i.redd.it/ik47wnugcrsd1.jpeg\n",
      "\n",
      "My jank 2x 3090, 1x a4000 setup\n",
      "https://www.reddit.com/gallery/1fwk815\n",
      "I had room for the a4000 vertically but - it would cut the air to one of the 3090s and itself, so out rigged up a bracket with a pcie x16 extender and of course I had to use one cable tie to make it locallama worthy ;)\n",
      "llama-swap: a proxy for llama.cpp to swap between models\n",
      "https://github.com/mostlygeek/llama-swap\n",
      "\n",
      "Wake up babe, ZLUDA's alive again\n",
      "https://vosen.github.io/ZLUDA/blog/zludas-third-life/\n",
      "\n",
      "Drummer's Tiger Gemma 9B v3 - Decensored differently.\n",
      "https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v3\n",
      "\n",
      "Reflection 70B Reproduction Failed\n",
      "https://x.com/mattshumer_/status/1842313328166907995\n",
      "\n",
      "Would you use a Pinterest for UI/UX code? Prototyped this tonight out of my own need for a project\n",
      "https://v.redd.it/outifbt39vsd1\n",
      "\n",
      "Just another local inference build and its challenges\n",
      "https://i.redd.it/5mvt1r93mwsd1.jpeg\n",
      "Flexing my double RTX 3090 build. Had occasional boot issues but resolved it by dropping PCIe gen from 4 to 3, despite riser being right for the job. Still need to find a method to mount the front card in a more trustworthy way. Btw I am not crazy enough to buy them from the store so got used ones for just below 1000 USD. Spare me noting that I should change my watercooling pipes, ikr :D I‚Äôm inferring locally for my own AI project, as a replacement for Copilot (autocompletion for programming) and also I can load NDA covered documents without worrying about it. Llama models are king now and I use them for most of listed purposes. \n",
      "Build your real life emotional companion in few dollars\n",
      "https://github.com/StarmoonAI/Starmoon\n",
      "HN link: https://news.ycombinator.com/item?id=41743546\n",
      "Raspberry Pi 5 vs Orange Pi 5B: Test of LocalLLM Performance\n",
      "https://youtu.be/OXSsrWpIm8o\n",
      "\n",
      "so what happened to the wizard models, actually? was there any closure? did they get legally and academically assassinated? how? because i woke up at 4am thinking about this \n",
      "https://i.redd.it/za3osyhwpnsd1.png\n",
      "\n",
      "Gemma 2 2b-it is an underrated SLM GOAT\n",
      "https://i.redd.it/18x465phhnsd1.png\n",
      "\n",
      "Gentle continued lighthearted prodding. Love these devs. We‚Äôre all rooting for you! \n",
      "https://i.redd.it/uhqultuj8lsd1.jpeg\n",
      "\n",
      "Does USA have free online legal databases in the same format as BAILII, CanLII, CommonLII, etc?\n",
      "https://law.stackexchange.com/q/93639/41\n",
      "\n",
      "Fine-Tuning Insights: Lessons from Experimenting with RedPajama Large Language Model on Flyte Slack Data\n",
      "https://www.union.ai/blog-post/fine-tuning-insights-lessons-from-experimenting-with-redpajama-large-language-model-on-flyte-slack-data\n",
      "\n",
      "Falcon 40B - impressive local LLM\n",
      "https://www.youtube.com/watch?v=-IV1NTGy6Mg\n",
      "\n",
      "Introduction to Language Models (LLM's, Prompt Engineering, Encoder/Deco...\n",
      "https://youtube.com/watch?v=9PGmMdkTZls&feature=share\n",
      "\n",
      "NSQL: First Ever Fully OpenSource SQL Foundation Model\n",
      "https://ithinkbot.com/nsql-first-ever-fully-opensource-sql-foundation-model-f7b501d91ca4\n",
      "\n",
      "Running LLM As Chatbot in your cloud (AWS/GCP/Azure) with a single command\n",
      "https://github.com/dstackai/LLM-As-Chatbot/wiki/Running-LLM-As-Chatbot-in-your-cloud\n",
      "\n",
      "INTRODUCING SECONDBRAIN: AN OPEN-SOURCE WEB APP THAT SERVES AS AN ALTERNATIVE TO OPENAI. WITH THE ABILITY TO ADD CUSTOM KNOWLEDGE USING PDF, SOURCE LINKS, AND WIKIPEDIA, SECONDBRAIN OFFERS PERSONALIZED ASSISTANCE ALONGSIDE INTELLIGENT RESPONSES, MAKING IT AN INVALUABLE TOOL FOR RESEARCH, WRITING etc\n",
      "https://v.redd.it/i1x8ryto1e8b1\n",
      "\n",
      "Two mini-courses for chatting with Github and Arxiv using LangChainAI, OpenAI, ChromaDB, Pinecone and ArizeAI\n",
      "/r/LLMDevs/comments/14hpzj7/two_minicourses_for_chatting_with_github_and/\n",
      "\n",
      "What is the approach for extraction of structured data from financial documents\n",
      "/r/LocalLLaMA/comments/14g691g/what_is_the_approach_for_extraction_of_structured/\n",
      "\n",
      "ChatGPT-based Research Pilot\n",
      "https://researchpilot.fly.dev\n",
      "Hey guys, I‚Äôve been working on a research tool that provides information and analysis on recent events. I wasn‚Äôt impressed with what was currently available so I developed one myself.\n",
      "\n",
      "Here‚Äôs the site: https://researchpilot.fly.dev\n",
      "\n",
      "I based the architecture on this paper:\n",
      "https://arxiv.org/abs/2212.10496\n",
      "\n",
      "It‚Äôs free to use and doesn‚Äôt require a user account. I hope it‚Äôs useful, and I‚Äôm still adding features and capabilities.\n",
      "\n",
      "I‚Äôd love to hear feedback if you guys use it!\n",
      "google course on generative ai\n",
      "https://youtube.com/watch?v=1wXnYyLzjhI&feature=share\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    if 'https://www.reddit.com/r' not in doc.metadata['post_url']:\n",
    "        print(doc.metadata['post_title'])\n",
    "        print(doc.metadata['post_url'])\n",
    "        print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-mlops-gR6V-zHB-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
